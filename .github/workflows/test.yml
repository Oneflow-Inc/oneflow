name: Build and Test CI
on:
  pull_request:
    types: [opened, review_requested, ready_for_review, synchronize, unlocked]

concurrency:
  group: build-and-test-${{ github.ref }}
  cancel-in-progress: true

env:
  OSS_ACCESS_KEY_ID: ${{ secrets.OSS_ACCESS_KEY_ID }}
  OSS_ACCESS_KEY_SECRET: ${{ secrets.OSS_ACCESS_KEY_SECRET }}
  ONEFLOW_TIMEOUT_SECONDS: 90
  FLOW_VISION_SRC: flow_vision
  TEST_WITH_TORCH_IMG_TAG: registry.cn-beijing.aliyuncs.com/oneflow/test-with-pytorch-1.9.0-cuda10.2-cudnn7-runtime:9b7ac2af8823bf537a636f8589fd60f51d4af348

jobs:
  check-priority-pr:
    name: "Check priority PR"
    runs-on: ubuntu-latest
    if: github.event.pull_request.draft == false && github.base_ref == 'master' && contains(github.event.pull_request.requested_reviewers.*.login, 'oneflow-ci-bot')
    steps:
      - uses: Oneflow-Inc/get-oneflow/priority-pr@support-auto-benchmark
        name: Check priority PR closed
        id: save-cache
        timeout-minutes: 5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

  mirror_third_party:
    name: Mirror third party dependencies
    runs-on: ubuntu-18.04
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    needs: [check-priority-pr]
    steps:
      - uses: actions/checkout@v2
      - name: Mirror dependencies to aliyun
        if: github.event.pull_request.head.repo.full_name == github.repository
        run: |
          set -x
          if [ -z "$OSS_ACCESS_KEY_ID" ]
          then
            exit 0
          fi
          python3 -m pip install -U pip setuptools wheel
          python3 -m pip install oss2
          python3 tools/package_mirror.py -i $PWD

  check_license_and_format:
    name: License and format
    runs-on: ubuntu-18.04
    if: github.event.pull_request.draft == false
    needs: [check-priority-pr]
    steps:
      - uses: actions/checkout@v2
        with:
          repository: ${{github.event.pull_request.head.repo.full_name}}
          ref: ${{ github.head_ref }}
      - name: Check license
        id: license_check
        run: |
          python3 ci/check/run_license_format.py -i oneflow -c
          python3 ci/check/run_license_format.py -i python -c
      - name: Add license
        id: license_fmt
        if: ${{ failure() }}
        run: |
          python3 ci/check/run_license_format.py -i oneflow --fix
          python3 ci/check/run_license_format.py -i python --fix
      - name: Check C++/CUDA format
        id: cpp_check
        run: |
          python3 ci/check/run_clang_format.py --clang_format_binary clang-format --source_dir oneflow
      - name: Run C++/CUDA format
        id: cpp_fmt
        if: ${{ failure() }}
        run: |
          python3 ci/check/run_clang_format.py --clang_format_binary clang-format --source_dir oneflow --fix
      - name: Check Python format
        id: py_check
        run: |
          python3 -m pip install black==19.10b0
          python3 ci/check/run_py_format.py --source_dir $PWD
      - name: Run Python Format
        id: py_fmt
        if: ${{ failure() }}
        run: |
          python3 -m pip install black==19.10b0
          python3 ci/check/run_py_format.py --source_dir $PWD --fix
      - name: Check CMake format
        id: cmake_check
        run: |
          python3 -m pip install cmakelang
          python3 ci/check/run_cmake_format.py --source_dir $PWD
      - name: Run CMake Format
        id: cmake_fmt
        if: ${{ failure() }}
        run: |
          python3 -m pip install cmakelang
          python3 ci/check/run_cmake_format.py --source_dir $PWD --fix
      - name: Git push
        id: git_push
        if: ${{ failure() }}
        run: |
          git diff -p > license_and_format.patch
          cat license_and_format.patch
          git config --global user.email "ci-bot@oneflow.org"
          git config --global user.name "oneflow-ci-bot"
          git add -u
          git commit -m "auto format by CI"
          git push
      - name: Upload patch
        if: ${{ failure() && steps.git_push.outcome == 'failure' }}
        uses: actions/upload-artifact@v2
        with:
          name: license_and_format-${{ github.sha }}.patch
          path: license_and_format.patch
      - name: Add comment
        if: ${{ failure() }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'Code got formatted by CI. Please request CI again if you still want to have this PR merged. If the PR is from a forked repo, please download the patch files from the GitHub Actions web page and apply them locally.'
            })
      - name: Please request CI again
        if: ${{ failure() }}
        run: |
          exit 1
      - name: Check source code (prevent creating files at wrong places)
        run: |
          python3 tools/check_src.py

  wait_for_gpu_slot:
    name: Wait for GPU slots
    runs-on: ubuntu-latest
    concurrency:
      group: build-and-test-wait-for-gpu-slot
      cancel-in-progress: false
    needs: [build-oneflow]
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    continue-on-error: true
    steps:
      - name: Check if secrets accessible
        env:
          CI_PERSONAL_ACCESS_TOKEN: ${{ secrets.CI_PERSONAL_ACCESS_TOKEN }}
        run: |
          set -x
          if [ -z "$CI_PERSONAL_ACCESS_TOKEN" ]
          then
            exit 0
          fi
          echo "is_secrets_accessible=1" >> $GITHUB_ENV
      - name: Wait for GPU slot
        uses: Oneflow-Inc/get-oneflow/wait-for-gpu@support-auto-benchmark
        if: env.is_secrets_accessible == '1'
        timeout-minutes: 90
        continue-on-error: true
        with:
          token: ${{ secrets.CI_PERSONAL_ACCESS_TOKEN }}
          timeout-minutes: 3
          max-try-times: 30

  find-build-cache:
    name: "Find build cache"
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    runs-on: ubuntu-latest
    needs: [check-priority-pr]
    env:
      ONEFLOW_SRC: .
    outputs:
      matrix: ${{ steps.find-cache.outputs.matrix }}
    steps:
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - uses: Oneflow-Inc/get-oneflow/cache-complete/matrix/build@support-auto-benchmark
        name: find cache
        id: find-cache
        timeout-minutes: 5
        with:
          delete-cache: ${{ contains(github.event.pull_request.labels.*.name, 'need-clean-ccache') }}
          runner-labels: |
            self-hosted
            linux
            build
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          entries: |
            cu102
            cpu
            cu102_xla
            llvm13
            openvino

  build-oneflow:
    name: "Build OneFlow"
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    runs-on: ${{ matrix.runs-on }}
    needs: [find-build-cache]
    timeout-minutes: 60
    strategy:
      fail-fast: true
      max-parallel: 5
      matrix: ${{ fromJson(needs.find-build-cache.outputs.matrix) }}
    env:
      ONEFLOW_SRC: .
      MANYLINUX_CACHE_DIR: ~/manylinux-cache-dir/${{ matrix.entry }}
      WHEELHOUSE_DIR: manylinux-wheelhouse
      SSH_TANK_HOST: 192.168.1.13
      SSH_TANK_PATH: /tank
    steps:
      - name: Fix permissions
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *
      - name: Remove leftover cuda-installer.log
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker run --rm -v /tmp:/host/tmp -w /p busybox rm -f /host/tmp/cuda-installer.log
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - uses: Oneflow-Inc/get-oneflow/cache-complete@support-auto-benchmark
        name: Save cache if successful
        id: save-cache
        timeout-minutes: 5
        with:
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          entry: ${{ matrix.entry }}
          digest-type: build
          mark-as-completed: ${{ contains(matrix.runs-on, 'self-hosted') && github.event.pull_request.head.repo.full_name == github.repository }}
      - name: Check digest cache result. If this step failed, usually it is caused by new commits pushed when this CI run is running.
        if: ${{ fromJSON(steps.save-cache.outputs.cache-hit) != matrix.cache-hit }}
        run: |
          echo "::error file=test.yml,line=204,col=10::steps.save-cache.outputs.cache-hit != matrix.cache-hit"
          exit 1
      - uses: Oneflow-Inc/get-oneflow@support-auto-benchmark
        name: Build manylinux ${{ matrix.entry }}
        id: build-cpu
        if: ${{ matrix.entry =='cpu' && !matrix.cache-hit }}
        with:
          cmake-init-cache: ${{ env.ONEFLOW_SRC }}/cmake/caches/ci/cpu.cmake
          build-script: ${{ env.ONEFLOW_SRC }}/ci/manylinux/build.sh
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          oneflow-build-env: manylinux
          wheelhouse-dir: ${{ env.WHEELHOUSE_DIR }}
          clear-wheelhouse-dir: true
          self-hosted: ${{ contains(matrix.runs-on, 'self-hosted') }}
          cuda-version: none
          manylinux-cache-dir: ${{ env.MANYLINUX_CACHE_DIR }}
          docker-run-use-system-http-proxy: false
          docker-run-use-lld: true
          retry-failed-build: true
          clean-ccache: ${{ contains(github.event.pull_request.labels.*.name, 'need-clean-ccache') }}
          python-versions: |
            3.6
            3.7
      - uses: Oneflow-Inc/get-oneflow@support-auto-benchmark
        name: Build manylinux ${{ matrix.entry }}
        id: build-openvino
        if: ${{ matrix.entry =='openvino' && !matrix.cache-hit }}
        with:
          cmake-init-cache: ${{ env.ONEFLOW_SRC }}/cmake/caches/ci/serving/openvino.cmake
          build-script: ${{ env.ONEFLOW_SRC }}/ci/manylinux/build-gcc7.sh
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          oneflow-build-env: openvino
          wheelhouse-dir: ${{ env.WHEELHOUSE_DIR }}
          clear-wheelhouse-dir: true
          self-hosted: ${{ contains(matrix.runs-on, 'self-hosted') }}
          cuda-version: none
          manylinux-cache-dir: ${{ env.MANYLINUX_CACHE_DIR }}
          docker-run-use-system-http-proxy: false
          docker-run-use-lld: true
          retry-failed-build: true
          wheel-audit: false
          clean-ccache: ${{ contains(github.event.pull_request.labels.*.name, 'need-clean-ccache') }}
          python-versions: |
            3.6
      - uses: Oneflow-Inc/get-oneflow@support-auto-benchmark
        name: Build manylinux ${{ matrix.entry }}
        id: build-cuda
        if: ${{ matrix.entry =='cu102' && !matrix.cache-hit }}
        with:
          cmake-init-cache: ${{ env.ONEFLOW_SRC }}/cmake/caches/ci/cuda.cmake
          build-script: ${{ env.ONEFLOW_SRC }}/ci/manylinux/build-gcc7.sh
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          oneflow-build-env: manylinux
          wheelhouse-dir: ${{ env.WHEELHOUSE_DIR }}
          clear-wheelhouse-dir: true
          self-hosted: ${{ contains(matrix.runs-on, 'self-hosted') }}
          cuda-version: "10.2"
          manylinux-cache-dir: ${{ env.MANYLINUX_CACHE_DIR }}
          docker-run-use-system-http-proxy: false
          docker-run-use-lld: false
          retry-failed-build: true
          clean-ccache: ${{ contains(github.event.pull_request.labels.*.name, 'need-clean-ccache') }}
          python-versions: |
            3.7
      - uses: Oneflow-Inc/get-oneflow@support-auto-benchmark
        name: Build manylinux ${{ matrix.entry }}
        id: build-xla
        if: ${{ matrix.entry =='cu102_xla' && !matrix.cache-hit }}
        with:
          cmake-init-cache: ${{ env.ONEFLOW_SRC }}/cmake/caches/ci/cuda-xla.cmake
          build-script: ${{ env.ONEFLOW_SRC }}/ci/manylinux/build-gcc7-xla.sh
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          oneflow-build-env: manylinux
          wheelhouse-dir: ${{ env.WHEELHOUSE_DIR }}
          clear-wheelhouse-dir: true
          self-hosted: ${{ contains(matrix.runs-on, 'self-hosted') }}
          cuda-version: "10.2"
          manylinux-cache-dir: ${{ env.MANYLINUX_CACHE_DIR }}
          docker-run-use-system-http-proxy: true
          docker-run-use-lld: true
          retry-failed-build: true
          clean-ccache: ${{ contains(github.event.pull_request.labels.*.name, 'need-clean-ccache') }}
          python-versions: |
            3.6
      - uses: Oneflow-Inc/get-oneflow@support-auto-benchmark
        name: Build ${{ matrix.entry }}
        if: ${{ matrix.entry == 'llvm13' && !matrix.cache-hit }}
        with:
          cmake-init-cache: ${{ env.ONEFLOW_SRC }}/cmake/caches/ci/llvm/cuda-75-clang.cmake
          build-script: ${{ env.ONEFLOW_SRC }}/ci/clang/build-llvm.sh
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          oneflow-build-env: llvm
          wheelhouse-dir: ${{ env.WHEELHOUSE_DIR }}
          clear-wheelhouse-dir: true
          self-hosted: true
          cuda-version: ${{ env.CUDA_VERSION }}
          manylinux-cache-dir: ${{ env.MANYLINUX_CACHE_DIR }}
          docker-run-use-system-http-proxy: false
          docker-run-use-lld: false
          retry-failed-build: true
          clean-ccache: ${{ contains(github.event.pull_request.labels.*.name, 'need-clean-ccache') }}
          wheel-audit: false
          python-versions: |
            3.8
      - name: Remove automerge
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') && cancelled() == false && contains(github.event.pull_request.labels.*.name, 'automerge') }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'automerge'
            })
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'CI failed when running job: Build ${{ matrix.entry }}. PR label automerge has been removed'
            })
      - name: Upload packed liboneflow
        if: ${{ !fromJson(matrix.cache-hit) && matrix.entry != 'llvm13' && matrix.entry != 'cu102_xla' }}
        uses: Oneflow-Inc/get-oneflow/digest/upload@support-auto-benchmark
        timeout-minutes: 10
        with:
          digest: ${{ steps.save-cache.outputs.build-digest }}
          entry: ${{ matrix.entry }}
          ssh-tank-host: ${{ env.SSH_TANK_HOST }}
          ssh-tank-path: ${{ env.SSH_TANK_PATH }}
          src-dir: ${{ env.MANYLINUX_CACHE_DIR }}/build/cpack
          dst-dir: cpack
      - name: Upload whl
        if: ${{ !fromJson(matrix.cache-hit) && matrix.entry != 'llvm13' && matrix.entry != 'cu102_xla' }}
        uses: Oneflow-Inc/get-oneflow/digest/upload@support-auto-benchmark
        timeout-minutes: 10
        with:
          digest: ${{ steps.save-cache.outputs.build-digest }}
          entry: ${{ matrix.entry }}
          ssh-tank-host: ${{ env.SSH_TANK_HOST }}
          ssh-tank-path: ${{ env.SSH_TANK_PATH }}
          src-dir: ${{ env.WHEELHOUSE_DIR }}
          dst-dir: whl

  find-test-cache-distributed:
    name: "Find test cache (distributed)"
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    runs-on: ubuntu-latest
    needs: [build-oneflow]
    env:
      ONEFLOW_SRC: .
    outputs:
      matrix: ${{ steps.find-cache.outputs.matrix }}
    steps:
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - uses: Oneflow-Inc/get-oneflow/cache-complete/matrix/test@support-auto-benchmark
        name: find cache
        id: find-cache
        timeout-minutes: 5
        with:
          runner-labels: |
            self-hosted
            linux
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          include-distributed: true
          world-size: 2
          devices: |
            cuda
          tests: |
            module

  find-test-cache:
    name: "Find test cache"
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    runs-on: ubuntu-latest
    needs: [build-oneflow]
    env:
      ONEFLOW_SRC: .
    outputs:
      matrix: ${{ steps.find-cache.outputs.matrix }}
    steps:
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - uses: Oneflow-Inc/get-oneflow/cache-complete/matrix/test@support-auto-benchmark
        name: find cache
        id: find-cache
        timeout-minutes: 5
        with:
          runner-labels: |
            self-hosted
            linux
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          devices: |
            cuda
          tests: |
            module
            misc
            speed-test

  find-benchmark-cache:
    name: "Find benchmark cache"
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    runs-on: ubuntu-latest
    needs: [build-oneflow]
    env:
      ONEFLOW_SRC: .
    outputs:
      matrix: ${{ steps.find-cache.outputs.matrix }}
    steps:
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - uses: Oneflow-Inc/get-oneflow/cache-complete/matrix/test@support-auto-benchmark
        name: find cache
        id: find-cache
        timeout-minutes: 5
        with:
          runner-labels: |
            self-hosted
            linux
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          devices: |
            cuda
          tests: |
            benchmark

  benchmark:
    name: Benchmark suite
    needs: [wait_for_gpu_slot, find-benchmark-cache]
    runs-on: ${{ matrix.runs-on }}
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    strategy:
      fail-fast: true
      max-parallel: 1
      matrix: ${{ fromJson(needs.find-benchmark-cache.outputs.matrix) }}
    env:
      ONEFLOW_SRC: .
      TEST_CONTAINER_NAME: "ci-benchmark"
      SSH_TANK_HOST: 192.168.1.13
      SSH_TANK_PATH: /tank
    steps:
      - name: Fix permissions
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf .pytest_cache
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - name: Remove container
        timeout-minutes: 45
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
      - uses: Oneflow-Inc/get-oneflow/cache-complete@support-auto-benchmark
        name: Save cache if successful
        id: save-cache
        timeout-minutes: 5
        with:
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          entry: ${{ matrix.entry }}
          digest-type: ${{ matrix.digest-type }}
          mark-as-completed: ${{ contains(matrix.runs-on, 'self-hosted') && github.event.pull_request.head.repo.full_name == github.repository }}
      - name: Check digest cache result. If this step failed, usually it is caused by new commits pushed when this CI run is running.
        if: ${{ fromJSON(steps.save-cache.outputs.cache-hit) != matrix.cache-hit }}
        run: |
          echo "::error file=test.yml,line=204,col=10::steps.save-cache.outputs.cache-hit != matrix.cache-hit"
          exit 1
      - name: Download wheel and packed liboneflow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: Oneflow-Inc/get-oneflow/digest/download@support-auto-benchmark
        id: download-digest
        timeout-minutes: 10
        with:
          digest: ${{ steps.save-cache.outputs.build-digest }}
          entry: ${{ matrix.compute-platform }}
          ssh-tank-host: ${{ env.SSH_TANK_HOST }}
          ssh-tank-path: ${{ env.SSH_TANK_PATH }}
      - name: Set environment variables
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          echo "ONEFLOW_TEST_CACHE_DIR=$HOME/ci-cache/test_cache" >> $GITHUB_ENV
          echo "ONEFLOW_WHEEL_PATH=${{ steps.download-digest.outputs.entry-dir }}/whl" >> $GITHUB_ENV
          echo "ONEFLOW_CPACK_PATH=${{ steps.download-digest.outputs.entry-dir }}/cpack" >> $GITHUB_ENV
      - name: Set environment variables (distributed)
        if: ${{ fromJson(matrix.is-distributed) }}
        run: |
          set -x
          EXTRA_DOCKER_ARGS+=" --network host "
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Enable ONEFLOW_TEST_VERBOSE
        if: ${{ contains(github.event.pull_request.labels.*.name, 'need-test-verbose') }}
        run: |
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_TEST_VERBOSE=1"
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Start container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        working-directory: ${{ env.ONEFLOW_SRC }}
        run: |
          docker run -d --rm --privileged --shm-size=8g \
            --pids-limit -1 \
            --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
            --runtime=nvidia \
            -v /dataset:/dataset:ro -v /model_zoo:/model_zoo:ro \
            -v ${ONEFLOW_WHEEL_PATH}:${ONEFLOW_WHEEL_PATH}:ro \
            -v $HOME/test-container-cache/dot-local:/root/.local \
            -v $HOME/test-container-cache/dot-cache:/root/.cache \
            -e ONEFLOW_WHEEL_PATH=${ONEFLOW_WHEEL_PATH} \
            -e ONEFLOW_CI=1 \
            -v $PWD:$PWD \
            -w $PWD \
            -v ${ONEFLOW_TEST_CACHE_DIR}:${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TEST_CACHE_DIR=${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TIMEOUT_SECONDS=${{ env.ONEFLOW_TIMEOUT_SECONDS }} \
            -e ONEFLOW_MLIR_ENABLE_ROUND_TRIP=1 \
            --name ${TEST_CONTAINER_NAME} \
            ${{ env.EXTRA_DOCKER_ARGS }} \
            ${{ env.TEST_WITH_TORCH_IMG_TAG }} \
            sleep 5400
      - name: Test container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} ls
      - name: Install OneFlow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          ls ${ONEFLOW_WHEEL_PATH}
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install --find-links=${ONEFLOW_WHEEL_PATH} oneflow
      - name: Checkout Oneflow-Inc/vision
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: actions/checkout@v2
        with:
          repository: Oneflow-Inc/vision
          # please use a commit here
          ref: 0a291a00167143e64c7f7e5743ea93bf6a50a6b6
          path: ${{ env.FLOW_VISION_SRC}}
      - name: Install Flow Vision
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install -e ${{ env.FLOW_VISION_SRC}}
      # start pytest benchmark
      - name: Benchmark alexnet
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_alexnet.py
          benchmark-id: 1-gpu-alexnet
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark convnext_tiny_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_convnext_tiny_224.py
          benchmark-id: 1-gpu-convnext_tiny_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark crossformer_tiny_patch4_group7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_crossformer_tiny_patch4_group7_224.py
          benchmark-id: 1-gpu-crossformer_tiny_patch4_group7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark cswin_tiny_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_cswin_tiny_224.py
          benchmark-id: 1-gpu-cswin_tiny_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark densent121
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_densent121.py
          benchmark-id: 1-gpu-densent121
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark ghostnet
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_ghostnet.py
          benchmark-id: 1-gpu-ghostnet
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark googlenet
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_googlenet.py
          benchmark-id: 1-gpu-googlenet
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark inception_v3
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_inception_v3.py
          benchmark-id: 1-gpu-inception_v3
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mlp_mixer_b16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mlp_mixer_b16_224.py
          benchmark-id: 1-gpu-mlp_mixer_b16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mnasnet0_5
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mnasnet0_5.py
          benchmark-id: 1-gpu-mnasnet0_5
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mobilenet_v2
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mobilenet_v2.py
          benchmark-id: 1-gpu-mobilenet_v2
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark mobilenet_v3
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_mobilenet_v3.py
          benchmark-id: 1-gpu-mobilenet_v3
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_m36
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_m36.py
          benchmark-id: 1-gpu-poolformer_m36
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_m48
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_m48.py
          benchmark-id: 1-gpu-poolformer_m48
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_s12
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_s12.py
          benchmark-id: 1-gpu-poolformer_s12
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_s24
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_s24.py
          benchmark-id: 1-gpu-poolformer_s24
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark poolformer_s36
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_poolformer_s36.py
          benchmark-id: 1-gpu-poolformer_s36
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark pvt_samll
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_pvt_samll.py
          benchmark-id: 1-gpu-pvt_samll
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark pvt_tiny
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_pvt_tiny.py
          benchmark-id: 1-gpu-pvt_tiny
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark res2net50_26w_4s
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_res2net50_26w_4s.py
          benchmark-id: 1-gpu-res2net50_26w_4s
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resmlp_12_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resmlp_12_224.py
          benchmark-id: 1-gpu-resmlp_12_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest101
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest101.py
          benchmark-id: 1-gpu-resnest101
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest200
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest200.py
          benchmark-id: 1-gpu-resnest200
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest269
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest269.py
          benchmark-id: 1-gpu-resnest269
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnest50
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnest50.py
          benchmark-id: 1-gpu-resnest50
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnet50
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnet50.py
          benchmark-id: 1-gpu-resnet50
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark resnext50_32x4d
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_resnext50_32x4d.py
          benchmark-id: 1-gpu-resnext50_32x4d
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark rexnet_lite_1_0
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_rexnet_lite_1_0.py
          benchmark-id: 1-gpu-rexnet_lite_1_0
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark rexnetv1_1_0
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_rexnetv1_1_0.py
          benchmark-id: 1-gpu-rexnetv1_1_0
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark shufflenet_v2_x0_5
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_shufflenet_v2_x0_5.py
          benchmark-id: 1-gpu-shufflenet_v2_x0_5
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark squeezenet1_0
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_squeezenet1_0.py
          benchmark-id: 1-gpu-squeezenet1_0
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark swin_base_patch4_window7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_swin_base_patch4_window7_224.py
          benchmark-id: 1-gpu-swin_base_patch4_window7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark swin_small_patch4_window7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_swin_small_patch4_window7_224.py
          benchmark-id: 1-gpu-swin_small_patch4_window7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark swin_tiny_patch4_window7_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_swin_tiny_patch4_window7_224.py
          benchmark-id: 1-gpu-swin_tiny_patch4_window7_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_base_ls
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_base_ls.py
          benchmark-id: 1-gpu-uniformer_base_ls
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_base
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_base.py
          benchmark-id: 1-gpu-uniformer_base
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_small_plus
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_small_plus.py
          benchmark-id: 1-gpu-uniformer_small_plus
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark uniformer_small
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_uniformer_small.py
          benchmark-id: 1-gpu-uniformer_small
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark vit_base_path16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_vit_base_path16_224.py
          benchmark-id: 1-gpu-vit_base_path16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark vit_small_path16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_vit_small_path16_224.py
          benchmark-id: 1-gpu-vit_small_path16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark vit-tiny_path16_224
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_vit-tiny_path16_224.py
          benchmark-id: 1-gpu-vit-tiny_path16_224
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      - name: Benchmark wide_resnet50_2
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'benchmark' && matrix.device == 'cuda' }}
        uses: Oneflow-Inc/get-oneflow/pytest-benchmark@support-auto-benchmark
        with:
          pytest-script: ${{ env.FLOW_VISION_SRC }}/benchmark/test_wide_resnet50_2.py
          benchmark-id: 1-gpu-wide_resnet50_2
          pytest-args: |
            -v
          pytest-compare-args: |
            --benchmark-compare-fail=min:5%
            --benchmark-compare-fail=mean:0.001
          container-name: ${{ env.TEST_CONTAINER_NAME }}
      # end pytest benchmark
      - name: Print stacks in all core files
        timeout-minutes: 45
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/print_stack_in_all_dirs.sh || true
      - name: Remove automerge
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') && cancelled() == false && contains(github.event.pull_request.labels.*.name, 'automerge') }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'automerge'
            })
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'CI failed when running job: ${{ matrix.entry }}. PR label automerge has been removed'
            })
      - name: Remove container
        timeout-minutes: 45
        if: ${{ always() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *

  test-distributed:
    name: Distributed test suite
    needs: [wait_for_gpu_slot, find-test-cache-distributed]
    runs-on: ${{ matrix.runs-on }}
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    concurrency:
      group: distributed-test-${{ matrix.entry }}-rank-${{ matrix.rank }}
      cancel-in-progress: false
    strategy:
      fail-fast: true
      max-parallel: 2
      matrix: ${{ fromJson(needs.find-test-cache-distributed.outputs.matrix) }}
    env:
      ONEFLOW_SRC: .
      TEST_CONTAINER_NAME: "ci-test-distributed"
      SSH_TANK_HOST: 192.168.1.13
      SSH_TANK_PATH: /tank
    steps:
      - name: Fix permissions
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf .pytest_cache
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - name: Remove container
        timeout-minutes: 45
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
      - uses: Oneflow-Inc/get-oneflow/cache-complete@support-auto-benchmark
        name: Save cache if successful
        id: save-cache
        timeout-minutes: 5
        with:
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          entry: ${{ matrix.entry }}
          digest-type: ${{ matrix.digest-type }}
          mark-as-completed: ${{ contains(matrix.runs-on, 'self-hosted') && github.event.pull_request.head.repo.full_name == github.repository }}
      - name: Check digest cache result. If this step failed, usually it is caused by new commits pushed when this CI run is running.
        if: ${{ fromJSON(steps.save-cache.outputs.cache-hit) != matrix.cache-hit }}
        run: |
          echo "::error file=test.yml,line=204,col=10::steps.save-cache.outputs.cache-hit != matrix.cache-hit"
          exit 1
      - name: Download wheel and packed liboneflow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: Oneflow-Inc/get-oneflow/digest/download@support-auto-benchmark
        id: download-digest
        timeout-minutes: 10
        with:
          digest: ${{ steps.save-cache.outputs.build-digest }}
          entry: ${{ matrix.compute-platform }}
          ssh-tank-host: ${{ env.SSH_TANK_HOST }}
          ssh-tank-path: ${{ env.SSH_TANK_PATH }}
      - name: Get primary node
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: Oneflow-Inc/get-oneflow/master-address@support-auto-benchmark
        id: get-primary-node
        with:
          rank: ${{ matrix.rank }}
      - name: Set environment variables
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          extra_docker_args=""
          if [ "${{ matrix.device }}" == "cpu" ]; then
            extra_docker_args+=" --env ONEFLOW_TEST_CPU_ONLY=1"
            extra_docker_args+=" --env CUDA_VISIBLE_DEVICES=-1"
          fi
          echo "EXTRA_DOCKER_ARGS=${extra_docker_args}" >> $GITHUB_ENV
          echo "ONEFLOW_TEST_CACHE_DIR=$HOME/ci-cache/test_cache" >> $GITHUB_ENV

          echo "ONEFLOW_WHEEL_PATH=${{ steps.download-digest.outputs.entry-dir }}/whl" >> $GITHUB_ENV
          echo "ONEFLOW_CPACK_PATH=${{ steps.download-digest.outputs.entry-dir }}/cpack" >> $GITHUB_ENV
      - name: Set environment variables (distributed)
        if: ${{ fromJson(matrix.is-distributed) }}
        run: |
          set -x
          EXTRA_DOCKER_ARGS+=" --network host "
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Enable ONEFLOW_TEST_VERBOSE
        if: ${{ contains(github.event.pull_request.labels.*.name, 'need-test-verbose') }}
        run: |
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_TEST_VERBOSE=1"
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Start container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        working-directory: ${{ env.ONEFLOW_SRC }}
        run: |
          docker run -d --rm --privileged --shm-size=8g \
            --pids-limit -1 \
            --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
            --runtime=nvidia \
            -v /dataset:/dataset:ro -v /model_zoo:/model_zoo:ro \
            -v ${ONEFLOW_WHEEL_PATH}:${ONEFLOW_WHEEL_PATH}:ro \
            -v $HOME/test-container-cache/dot-local:/root/.local \
            -v $HOME/test-container-cache/dot-cache:/root/.cache \
            -e NODE_RANK=${{ matrix.rank }} \
            -e _MASTER_ADDR=${{ steps.get-primary-node.outputs.master-address }} \
            -e ONEFLOW_WHEEL_PATH=${ONEFLOW_WHEEL_PATH} \
            -e ONEFLOW_CI=1 \
            -v $PWD:$PWD \
            -w $PWD \
            -v ${ONEFLOW_TEST_CACHE_DIR}:${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TEST_CACHE_DIR=${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TIMEOUT_SECONDS=${{ env.ONEFLOW_TIMEOUT_SECONDS }} \
            -e ONEFLOW_THRAED_LOCAL_CACHED_SIZE=${{ env.ONEFLOW_THRAED_LOCAL_CACHED_SIZE }} \
            -e ONEFLOW_MLIR_ENABLE_ROUND_TRIP=1 \
            --name ${TEST_CONTAINER_NAME} \
            ${{ env.EXTRA_DOCKER_ARGS }} \
            ${{ env.TEST_WITH_TORCH_IMG_TAG }} \
            sleep 5400
      - name: Test container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} ls
      - name: Install OneFlow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          ls ${ONEFLOW_WHEEL_PATH}
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install --find-links=${ONEFLOW_WHEEL_PATH} oneflow
      - name: Module API test (distributed)
        timeout-minutes: 90
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'module' && matrix.device == 'cuda' && fromJson(matrix.is-distributed) }}
        continue-on-error: false
        run: |
          docker exec -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/modules ${{ env.TEST_CONTAINER_NAME }} bash ci/test/2node_op_test_multi_client.sh
      - name: Module API test (distributed, without IB)
        timeout-minutes: 60
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'module' && matrix.device == 'cuda' && fromJson(matrix.is-distributed) && contains(github.event.pull_request.labels.*.name, 'need-distributed-without-ib')}}
        continue-on-error: false
        run: |
          docker exec -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/modules \
            -e ONEFLOW_LIBIBVERBS_PATH=invalid_lib \
            -e ONEFLOW_CI_DEVICE_NUMS="4" \
            ${{ env.TEST_CONTAINER_NAME }} bash ci/test/2node_op_test_multi_client.sh
      - name: Print stacks in all core files
        timeout-minutes: 45
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/print_stack_in_all_dirs.sh || true
      - name: Remove automerge
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') && cancelled() == false && contains(github.event.pull_request.labels.*.name, 'automerge') }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'automerge'
            })
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'CI failed when running job: ${{ matrix.entry }}. PR label automerge has been removed'
            })
      - name: Remove container
        timeout-minutes: 45
        if: ${{ always() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *

  test:
    name: Test suite
    needs: [wait_for_gpu_slot, find-test-cache, test-distributed]
    runs-on: ${{ matrix.runs-on }}
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    strategy:
      fail-fast: ${{ !contains(github.event.pull_request.labels.*.name, 'need-all-tests-even-fail') }}
      max-parallel: 5
      matrix: ${{ fromJson(needs.find-test-cache.outputs.matrix) }}
    env:
      ONEFLOW_SRC: .
      TEST_CONTAINER_NAME: "pr-${{ github.event.pull_request.number }}-run-id-${{ github.run_id }}-${{ matrix.entry }}-test"
      TEST_WITH_TF_IMG_TAG: registry.cn-beijing.aliyuncs.com/oneflow/test-with-tf-2.3.0:2f831e9354298a11447578e869d983959feb046f
      SSH_TANK_HOST: 192.168.1.13
      SSH_TANK_PATH: /tank
      METRICS_DIR: metrics
    steps:
      - name: Fix permissions
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf .pytest_cache
      - name: Checkout Oneflow-Inc/oneflow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
      - name: Remove container
        timeout-minutes: 45
        if: ${{ contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
      - uses: Oneflow-Inc/get-oneflow/cache-complete@support-auto-benchmark
        name: Save cache if successful
        id: save-cache
        timeout-minutes: 5
        with:
          oneflow-src: ${{ env.ONEFLOW_SRC }}
          entry: ${{ matrix.entry }}
          digest-type: ${{ matrix.digest-type }}
          mark-as-completed: ${{ contains(matrix.runs-on, 'self-hosted') && github.event.pull_request.head.repo.full_name == github.repository }}
      - name: Check digest cache result. If this step failed, usually it is caused by new commits pushed when this CI run is running.
        if: ${{ fromJSON(steps.save-cache.outputs.cache-hit) != matrix.cache-hit }}
        run: |
          echo "::error file=test.yml,line=204,col=10::steps.save-cache.outputs.cache-hit != matrix.cache-hit"
          exit 1
      - name: Download wheel and packed liboneflow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: Oneflow-Inc/get-oneflow/digest/download@support-auto-benchmark
        id: download-digest
        timeout-minutes: 10
        with:
          digest: ${{ steps.save-cache.outputs.build-digest }}
          entry: ${{ matrix.compute-platform }}
          ssh-tank-host: ${{ env.SSH_TANK_HOST }}
          ssh-tank-path: ${{ env.SSH_TANK_PATH }}
      - name: Enable TF container
        if: ${{ fromJSON(matrix.is-single-client) }}
        run: |
          echo "TEST_IMG_TAG=${TEST_WITH_TF_IMG_TAG}" >> $GITHUB_ENV
      - name: Enable Pytorch container
        if: ${{ !fromJSON(matrix.is-single-client) }}
        run: |
          echo "TEST_IMG_TAG=${TEST_WITH_TORCH_IMG_TAG}" >> $GITHUB_ENV
      - name: Set environment variables
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          set -x
          extra_docker_args=""
          if [ "${{ matrix.device }}" == "cpu" ]; then
            extra_docker_args+=" --env ONEFLOW_TEST_CPU_ONLY=1"
            extra_docker_args+=" --env CUDA_VISIBLE_DEVICES=-1"
          fi
          echo "EXTRA_DOCKER_ARGS=${extra_docker_args}" >> $GITHUB_ENV
          echo "ONEFLOW_TEST_CACHE_DIR=$HOME/ci-cache/test_cache" >> $GITHUB_ENV

          echo "ONEFLOW_WHEEL_PATH=${{ steps.download-digest.outputs.entry-dir }}/whl" >> $GITHUB_ENV
          echo "ONEFLOW_CPACK_PATH=${{ steps.download-digest.outputs.entry-dir }}/cpack" >> $GITHUB_ENV
      - name: Set environment variables (experimental flags)
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') && fromJson(matrix.is-experimental) }}
        run: |
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_KERNEL_ENABLE_CUDA_GRAPH=1"
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_THREAD_ENABLE_LOCAL_MESSAGE_QUEUE=1"
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_KERNEL_DISABLE_BLOB_ACCESS_CHECKER=1"
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Enable ONEFLOW_TEST_VERBOSE
        if: ${{ contains(github.event.pull_request.labels.*.name, 'need-test-verbose') }}
        run: |
          EXTRA_DOCKER_ARGS+=" --env ONEFLOW_TEST_VERBOSE=1"
          echo "EXTRA_DOCKER_ARGS=${EXTRA_DOCKER_ARGS}" >> $GITHUB_ENV
      - name: Pull image
        continue-on-error: true
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker pull ${{ env.TEST_IMG_TAG }}
      - name: Unzip packed liboneflow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') && !fromJson(matrix.is-xla) }}
        run: |
          unzip ${{ env.ONEFLOW_CPACK_PATH }}/liboneflow-ci-linux.zip
      - name: Start container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        working-directory: ${{ env.ONEFLOW_SRC }}
        run: |
          docker run -d --rm --privileged --shm-size=8g \
            --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
            --runtime=nvidia \
            -v /dataset:/dataset:ro -v /model_zoo:/model_zoo:ro \
            -v ${ONEFLOW_WHEEL_PATH}:${ONEFLOW_WHEEL_PATH}:ro \
            -v $HOME/test-container-cache/dot-local:/root/.local \
            -v $HOME/test-container-cache/dot-cache:/root/.cache \
            -e ONEFLOW_WHEEL_PATH=${ONEFLOW_WHEEL_PATH} \
            -e ONEFLOW_CI=1 \
            -v $PWD:$PWD \
            -w $PWD \
            -v ${ONEFLOW_TEST_CACHE_DIR}:${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TEST_CACHE_DIR=${ONEFLOW_TEST_CACHE_DIR} \
            -e ONEFLOW_TIMEOUT_SECONDS=${{ env.ONEFLOW_TIMEOUT_SECONDS }} \
            -e ONEFLOW_THRAED_LOCAL_CACHED_SIZE=${{ env.ONEFLOW_THRAED_LOCAL_CACHED_SIZE }} \
            -e ONEFLOW_MLIR_ENABLE_ROUND_TRIP=1 \
            --name ${TEST_CONTAINER_NAME} \
            ${{ env.EXTRA_DOCKER_ARGS }} \
            ${{ env.TEST_IMG_TAG }} \
            sleep 3600
      - name: Exe test
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' }}
        timeout-minutes: 10
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} ./liboneflow-ci-linux/bin/oneflow_testexe
      - name: Exe test (C++ API)
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' }}
        timeout-minutes: 10
        run: |
          docker exec -e ONEFLOW_SERVING_DEBUG=1 ${{ env.TEST_CONTAINER_NAME }} ./liboneflow-ci-linux/bin/oneflow_cpp_api_testexe
      - name: Test container
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} ls
      - name: Checkout Oneflow-Inc/vision
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: actions/checkout@v2
        with:
          repository: Oneflow-Inc/vision
          # please use a commit here
          ref: 0a291a00167143e64c7f7e5743ea93bf6a50a6b6
          path: ${{ env.FLOW_VISION_SRC}}
      - name: Checkout Oneflow-Inc/libai
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        uses: actions/checkout@v2
        with:
          repository: Oneflow-Inc/libai
          # please use a commit here
          ref: 7d31d9781e5f2d559dc0820f599e0bed798488ca
          path: ${{ env.LIBAI_SRC}}
      - name: Install OneFlow
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          ls ${ONEFLOW_WHEEL_PATH}
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install --find-links=${ONEFLOW_WHEEL_PATH} oneflow
      - name: Install downstream libs
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install -e ${{ env.FLOW_VISION_SRC}}
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install pybind11 --user
          docker exec ${TEST_CONTAINER_NAME} python3 -m pip install -e ${{ env.LIBAI_SRC}}
      - name: Run OneFlow doctor
        if: ${{ !fromJson(matrix.cache-hit) && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} python3 -m oneflow --doctor
      - name: Build documentation
        timeout-minutes: 10
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' && matrix.device == 'cpu' }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/build_docs.sh
      - name: Doctest
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' && matrix.device == 'cuda' }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/doctest.sh
      - name: Checkout Oneflow-Inc/models
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'speed-test' && matrix.device == 'cuda' }}
        uses: actions/checkout@v2
        with:
          repository: Oneflow-Inc/models
          ref: fff70995db75d11081037e60bad3d8215a8043d5
          path: oneflow-models
      - name: ResNet50 Graph DDP test
        id: models-resnet50
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'speed-test' && matrix.device == 'cuda' }}
        run: |
          docker exec -e ONEFLOW_MODELS_DIR=$PWD/oneflow-models ${{ env.TEST_CONTAINER_NAME }} bash ci/test/test_resnet50_graph_ddp.sh
      - name: Speed test
        id: speed
        timeout-minutes: 20
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'speed-test' && matrix.device == 'cuda' }}
        run: |
          docker exec -e ONEFLOW_MODELS_DIR=$PWD/oneflow-models ${{ env.TEST_CONTAINER_NAME }} bash ci/test/test_speed_multi_client.sh
      - name: Save speed stats
        if: ${{ always() && !fromJson(matrix.cache-hit) && matrix.test-type == 'speed-test' && matrix.device == 'cuda' }}
        run: |
          mkdir -p ${{ env.METRICS_DIR }}
          echo "${{ steps.speed.outputs.stats }}" >> ${{ env.METRICS_DIR }}/speed_stats.txt
      - name: Upload speed stats
        if: ${{ always() && !fromJson(matrix.cache-hit) && matrix.test-type == 'speed-test' && matrix.device == 'cuda' }}
        # must succeed if it is a branch of Oneflow-Inc/oneflow
        continue-on-error: ${{ !(github.repository == 'Oneflow-Inc/oneflow') }}
        uses: ./.github/actions/upload_oss
        with:
          src_path: ${{ env.METRICS_DIR }}
          oss_dst_path: oss://oneflow-log/${{ github.repository }}/metrics/pr/${{ github.event.pull_request.number }}/${{ github.event.pull_request.head.sha }}/${{github.run_id}}
          oss_access_key_id: ${{ secrets.OSS_ACCESS_KEY_ID }}
          oss_access_key_secret: ${{ secrets.OSS_ACCESS_KEY_SECRET }}
      - name: Post speed stats
        if: ${{ always() && !fromJson(matrix.cache-hit) && matrix.test-type == 'speed-test' && matrix.device == 'cuda' }}
        continue-on-error: true
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: "<details>\n <summary>Speed stats:</summary>\n\n ``` \n${{ steps.speed.outputs.stats }}\n ``` \n\n</details>".replace(/\\n/g, '\n')
            })
      - name: Module API test
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'module' && !fromJson(matrix.is-distributed) }}
        run: |
          docker exec -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/modules ${{ env.TEST_CONTAINER_NAME }} bash ci/test/generic_test_multi_client.sh
      - name: Graph API test
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' }}
        run: |
          docker exec -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/graph ${{ env.TEST_CONTAINER_NAME }} bash ci/test/generic_test_multi_client.sh
          docker exec ${{ env.TEST_CONTAINER_NAME }} python3 -m oneflow.distributed.launch --nproc_per_node 8 $PWD/python/oneflow/test/graph/test_neq_device_process_num.py
      - name: libai test
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' && matrix.device == 'cuda' }}
        run: |
          docker exec -e ONEFLOW_TEST_DEVICE_NUM=4 -w $PWD/${{ env.LIBAI_SRC }} ${{ env.TEST_CONTAINER_NAME }} python3 -m oneflow.distributed.launch --nproc_per_node 4 -m unittest -f tests/models/test_bert.py 
          docker exec -e ONEFLOW_TEST_DEVICE_NUM=4 -w $PWD/${{ env.LIBAI_SRC }} ${{ env.TEST_CONTAINER_NAME }} python3 -m oneflow.distributed.launch --nproc_per_node 4 -m unittest -f tests/models/test_gpt.py 
          docker exec -e ONEFLOW_TEST_DEVICE_NUM=4 -w $PWD/${{ env.LIBAI_SRC }} ${{ env.TEST_CONTAINER_NAME }} python3 -m oneflow.distributed.launch --nproc_per_node 4 -m unittest -f tests/models/test_t5.py 
          docker exec -e ONEFLOW_TEST_DEVICE_NUM=4 -w $PWD/${{ env.LIBAI_SRC }} ${{ env.TEST_CONTAINER_NAME }} python3 -m oneflow.distributed.launch --nproc_per_node 4 -m unittest -f tests/models/test_vit.py
      - name: Expensive tests (models, cases require exclusive access to GPU)
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && (matrix.test-type == 'speed-test' || (matrix.test-type == 'misc' && matrix.device == 'cpu')) && !fromJson(matrix.is-distributed) }}
        run: |
          docker exec \
            -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/expensive \
            ${{ env.TEST_CONTAINER_NAME }} bash ci/test/expensive_generic_test_multi_client.sh
      - name: Exception API test
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' }}
        run: docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/multi_client_exception_test.sh
      - name: Dataloader API test
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' }}
        run: |
          docker exec -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/dataloader ${{ env.TEST_CONTAINER_NAME }} bash ci/test/generic_test_multi_client.sh
      - name: Tensor API test
        timeout-minutes: 45
        if: ${{ !fromJson(matrix.cache-hit) && matrix.test-type == 'misc' }}
        run: |
          docker exec -e ONEFLOW_TEST_DIR=$PWD/python/oneflow/test/tensor ${{ env.TEST_CONTAINER_NAME }} bash ci/test/generic_test_multi_client.sh
      - name: Remove automerge
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') && cancelled() == false && contains(github.event.pull_request.labels.*.name, 'automerge') }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'automerge'
            })
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'CI failed when running job: ${{ matrix.entry }}. PR label automerge has been removed'
            })
      - name: Print stacks in all core files
        timeout-minutes: 45
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker exec ${{ env.TEST_CONTAINER_NAME }} bash ci/test/print_stack_in_all_dirs.sh || true
      - name: Query system status
        timeout-minutes: 45
        if: ${{ failure() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          nvidia-smi || true
          docker ps || true
      - name: Remove container
        timeout-minutes: 45
        if: ${{ always() && contains(matrix.runs-on, 'self-hosted') }}
        run: |
          docker rm -f ${{ env.TEST_CONTAINER_NAME }} || true
          docker run --rm -v $PWD:$PWD -w $PWD busybox rm -rf *

  static_analysis_with_clang_on_diff:
    name: Static analysis with clang on diff
    runs-on: ubuntu-20.04
    if: github.event.pull_request.draft == false && github.base_ref == 'master'
    needs: [check-priority-pr]
    steps:
      - name: Check out OneFlow
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          repository: ${{github.event.pull_request.head.repo.full_name}}
          fetch-depth: 0
      - uses: Oneflow-Inc/get-oneflow/cache-complete@support-auto-benchmark
        name: Save cache if successful
        id: save-cache
        timeout-minutes: 5
        with:
          oneflow-src: .
          entry: static_analysis_with_clang_on_diff
          digest-type: build
          mark-as-completed: ${{ github.event.pull_request.head.repo.full_name == github.repository }}
      - name: Install dependencies
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) }}
        run: |
          sudo apt-get update
          sudo apt-get install -y libopenblas-dev nasm python3-pip ninja-build ccache
      - name: Download OneFlow custom clang-tidy
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) }}
        run: |
          wget https://github.com/Oneflow-Inc/llvm-project/releases/download/llvmorg-13.0.0-maybe/clang-tidy-13.AppImage
          wget https://raw.githubusercontent.com/oneflow-inc/llvm-project/maybe/clang-tools-extra/clang-tidy/tool/clang-tidy-diff.py
          chmod +x clang-tidy-13.AppImage clang-tidy-diff.py
      - name: Cache third party dir
        uses: actions/cache@v2
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) }}
        with:
          path: ~/.ccache
          key: clang-tidy-diff-third-party-ccache-${{ hashFiles('**/CMakeLists.txt') }}-${{ hashFiles('**/*.cmake') }}
          restore-keys: |
            clang-tidy-diff-third-party-ccache-${{ hashFiles('**/CMakeLists.txt') }}-
            clang-tidy-diff-third-party-ccache-
      - name: Build third party libs and generate files
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) }}
        run: |
          export CCACHE_COMPRESS=true
          export CCACHE_MAXSIZE=500M
          mkdir build
          cd build
          cmake .. -C ../cmake/caches/international/cpu.cmake \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_TESTING=ON \
            -DCMAKE_C_COMPILER_LAUNCHER=ccache \
            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache
          cmake --build . -j$(nproc) --target oneflow_deps of_cfgobj of_protoobj of_functional_obj of_functional_tensor_obj of_op_schema
      - name: Fetch upstream
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) && github.event.pull_request.head.repo.full_name != github.event.pull_request.base.repo.full_name }}
        run: |
          git remote add upstream https://github.com/Oneflow-Inc/oneflow
          git fetch upstream
      - name: Run clang-tidy for modified files
        # use clang as compiler for correct compiler flags
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) }}
        run: |
          cd build
          rm CMakeCache.txt
          cmake .. -C ../cmake/caches/international/cpu.cmake \
            -DCMAKE_C_COMPILER=clang-12 \
            -DCMAKE_CXX_COMPILER=clang++-12 \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_TESTING=ON \
            -DCMAKE_EXPORT_COMPILE_COMMANDS=ON
          cd ..
          git diff -U0 ${{ github.event.pull_request.base.sha }} | ./clang-tidy-diff.py -clang-tidy-binary ./clang-tidy-13.AppImage -path build -allow-enabling-alpha-checkers -j $(nproc) -p1 -extra-arg="-Xclang" -extra-arg="-analyzer-config" -extra-arg="-Xclang" -extra-arg="aggressive-binary-operation-simplification=true" -warnings-as-errors="$(cat ./ci/check/clang_tidy_warnings_as_errors_on_diff)"
      - name: Remove automerge
        if: ${{ !fromJSON(steps.save-cache.outputs.cache-hit) && failure() && cancelled() == false && contains(github.event.pull_request.labels.*.name, 'automerge') }}
        uses: actions/github-script@v4
        with:
          script: |
            github.issues.removeLabel({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'automerge'
            })
            github.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'Static analysis with clang failed. PR label automerge has been removed'
            })
