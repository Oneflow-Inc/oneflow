From 877c71ff84d7259d062af4dfa009bc0db2f49437 Mon Sep 17 00:00:00 2001
From: hjchen2 <chenhoujiangcug@gmail.com>
Date: Thu, 17 Oct 2019 16:58:20 +0800
Subject: [PATCH] Fix WORKSPAC and add xla lib

---
 WORKSPACE                                          |   7 ++
 tensorflow/compiler/jit/xla_lib/BUILD              |  63 ++++++++++++
 .../compiler/jit/xla_lib/xla_runtime_util.cc       | 106 +++++++++++++++++++++
 tensorflow/compiler/jit/xla_lib/xla_runtime_util.h |  20 ++++
 4 files changed, 196 insertions(+)
 create mode 100644 tensorflow/compiler/jit/xla_lib/BUILD
 create mode 100644 tensorflow/compiler/jit/xla_lib/xla_runtime_util.cc
 create mode 100644 tensorflow/compiler/jit/xla_lib/xla_runtime_util.h

diff --git a/WORKSPACE b/WORKSPACE
index 74ea14d..b93c286 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -34,6 +34,13 @@ load(
 
 bazel_toolchains_repositories()
 
+http_archive(
+     name = "io_bazel_rules_docker",
+     sha256 = "aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d",
+     strip_prefix = "rules_docker-0.7.0",
+     urls = ["https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz"],
+ )
+
 load(
     "@io_bazel_rules_docker//repositories:repositories.bzl",
     container_repositories = "repositories",
diff --git a/tensorflow/compiler/jit/xla_lib/BUILD b/tensorflow/compiler/jit/xla_lib/BUILD
new file mode 100644
index 0000000..cd72955
--- /dev/null
+++ b/tensorflow/compiler/jit/xla_lib/BUILD
@@ -0,0 +1,63 @@
+licenses(["notice"])  # Apache 2.0
+
+load(
+    "//tensorflow:tensorflow.bzl",
+    "tf_cc_test",
+    "tf_cc_binary",
+    "tf_cc_shared_object",
+)
+load(
+    "//tensorflow/core:platform/default/build_config.bzl",
+    "tf_additional_all_protos",
+    "tf_proto_library",
+    "tf_proto_library_cc",
+    "tf_proto_library_py",
+)
+load("//tensorflow/compiler/xla:xla.bzl", "xla_proto_library")
+load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda")
+
+cc_library(
+    name = "xla_runtime_util",
+    srcs = [
+        "xla_runtime_util.cc",
+        "xla_runtime_util.h",
+    ],
+    visibility = ["//visibility:public"],
+    deps = [
+        "//tensorflow/stream_executor:stream",
+        "//tensorflow/compiler/xla/client:local_client",
+        "//tensorflow/compiler/xla/service/cpu:cpu_executable",
+        "//tensorflow/compiler/xla/service/gpu:gpu_executable",
+    ],
+)
+
+tf_cc_shared_object(
+    name = "libxla_core.so",
+    linkopts = select({
+        "//tensorflow:windows": [],
+        "//conditions:default": [
+            "-z muldefs",
+        ],
+    }),
+    visibility = ["//visibility:public"],
+    deps = [
+        ":xla_runtime_util",
+        "//tensorflow/compiler/jit:xla_tensor",
+        "//tensorflow/compiler/xla/client",
+        "//tensorflow/compiler/xla/client:xla_builder",
+        "//tensorflow/compiler/xla/client:xla_computation",
+        "//tensorflow/compiler/xla/client:executable_build_options",
+        "//tensorflow/compiler/xla/client:local_client",
+        "//tensorflow/compiler/xla/client:client_library",
+        "//tensorflow/compiler/xla/service",
+        "//tensorflow/compiler/xla/service:cpu_plugin",
+#        "//tensorflow/compiler/jit:xla_cpu_jit",
+        "//tensorflow/compiler/tf2xla:tf2xla_util",
+        "//tensorflow/compiler/tf2xla/lib:util",
+        "//tensorflow/core:lib",
+        "@com_google_absl//absl/strings",
+    ] + if_cuda([
+        "//tensorflow/compiler/xla/service:gpu_plugin",
+#        "//tensorflow/compiler/jit:xla_gpu_jit",
+    ]),
+)
diff --git a/tensorflow/compiler/jit/xla_lib/xla_runtime_util.cc b/tensorflow/compiler/jit/xla_lib/xla_runtime_util.cc
new file mode 100644
index 0000000..21f2f2d
--- /dev/null
+++ b/tensorflow/compiler/jit/xla_lib/xla_runtime_util.cc
@@ -0,0 +1,106 @@
+#include "tensorflow/stream_executor/gpu/gpu_stream.h"
+#include "tensorflow/compiler/xla/shape_tree.h"
+#include "tensorflow/compiler/xla/status_macros.h"
+#include "tensorflow/compiler/xla/service/hlo_value.h"
+#include "tensorflow/compiler/xla/service/hlo_instruction.h"
+#include "tensorflow/compiler/xla/service/cpu/cpu_executable.h"
+#include "tensorflow/compiler/xla/service/gpu/gpu_executable.h"
+
+#include "tensorflow/compiler/jit/xla_lib/xla_runtime_util.h"
+
+namespace xla {
+
+void SwapGpuStreamHandle(se::Stream *stream, void **gpu_stream) {
+  void **cuda_stream = se::gpu::AsGpuStream(stream)->GpuStreamMemberHack();
+  void *tmp_stream = *cuda_stream;
+  *cuda_stream = *gpu_stream;
+  *gpu_stream = tmp_stream;
+}
+
+inline size_t Align(int alignment, size_t size) {
+  return (size + alignment - 1) / alignment * alignment;
+}
+
+gpu::GpuExecutable *AsGpuExecutable(Executable *executable) {
+  return dynamic_cast<gpu::GpuExecutable *>(executable);
+}
+
+cpu::CpuExecutable *AsCpuExecutable(Executable *executable) {
+  return dynamic_cast<cpu::CpuExecutable *>(executable);
+}
+
+const BufferAssignment *GetBufferAssignment(Executable *executable) {
+  const BufferAssignment *assignment = nullptr;
+  if (auto *gpu_executable = AsGpuExecutable(executable)) {
+    assignment = (gpu_executable->GetBufferAssignment()).get();
+  } else if (auto *cpu_executable = AsCpuExecutable(executable)) {
+    assignment = &(cpu_executable->buffer_assignment());
+  } else {
+    LOG(FATAL) << "Only support CPU or GPU executable.";
+  }
+  return assignment;
+}
+
+size_t CalcWorkspaceByteSize(LocalExecutable *local_executable) {
+  const BufferAssignment *assignment =
+      GetBufferAssignment(local_executable->executable());
+  CHECK_NOTNULL(assignment);
+
+  size_t workspace_bytes = 0;
+  for (int i = 0; i < assignment->Allocations().size(); ++i) {
+    const BufferAllocation& allocation = assignment->GetAllocation(i);
+    // if (!allocation.is_entry_computation_parameter()) {
+    if (allocation.IsPreallocatedTempBuffer() || allocation.is_tuple()) {
+      workspace_bytes += Align(64/*alignment*/, allocation.size());
+    }
+  }
+  return workspace_bytes;
+}
+
+Status ResultAllocationIndices(LocalExecutable *local_executable,
+                               std::vector<int64_t> *indices) {
+  const BufferAssignment *assignment =
+      GetBufferAssignment(local_executable->executable());
+  CHECK_NOTNULL(assignment);
+
+  std::unordered_map<int, int> allocation_order;
+  for (int i = 0; i < assignment->Allocations().size(); ++i) {
+    const BufferAllocation& allocation = assignment->GetAllocation(i);
+    if ((allocation.maybe_live_out() ||
+        allocation.IsPreallocatedTempBuffer()) &&
+        !allocation.is_entry_computation_parameter()) {
+      allocation_order.emplace(i, allocation_order.size());
+    }
+  }
+
+  const HloModule &module = local_executable->executable()->module();
+  const HloInstruction* root = module.entry_computation()->root_instruction();
+  const InstructionValueSet &value_set =
+      assignment->dataflow_analysis().GetInstructionValueSet(root);
+
+  CHECK(root->shape().IsTuple());
+  int tuple_size = root->shape().tuple_shapes_size();
+  std::vector<int64_t> allocation_indices(tuple_size);
+  for (int i = 0; i < tuple_size; ++i) {
+    const auto& sources = value_set.element({i});
+    CHECK_EQ(1, sources.values().size());
+    auto instruction = sources.values()[0]->instruction();
+
+    TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice slice,
+                        assignment->GetUniqueSlice(
+                            instruction, sources.values()[0]->index()));
+    if (slice.allocation()->is_entry_computation_parameter()) {
+      // Output aliased with input will not be allocated, so we assign the
+      // allocation index -1
+      allocation_indices[i] = -1;
+    } else {
+      // Slice index is the allocation index
+      allocation_indices[i] = allocation_order.at(slice.index());
+    }
+  }
+
+  *indices = std::move(allocation_indices);
+  return Status::OK();
+}
+
+}  // namespace xla
diff --git a/tensorflow/compiler/jit/xla_lib/xla_runtime_util.h b/tensorflow/compiler/jit/xla_lib/xla_runtime_util.h
new file mode 100644
index 0000000..dd951f3
--- /dev/null
+++ b/tensorflow/compiler/jit/xla_lib/xla_runtime_util.h
@@ -0,0 +1,20 @@
+#ifndef TENSORFLOW_COMPILER_JIT_XLA_LIB_RUNTIME_WORKSPACE_BYTES_H_
+#define TENSORFLOW_COMPILER_JIT_XLA_LIB_RUNTIME_WORKSPACE_BYTES_H_
+
+#include <vector>
+
+#include "tensorflow/stream_executor/stream.h"
+#include "tensorflow/compiler/xla/client/local_client.h"
+
+namespace xla {
+
+void SwapGpuStreamHandle(se::Stream *stream, void **gpu_stream);
+
+size_t CalcWorkspaceByteSize(LocalExecutable *local_executable);
+
+Status ResultAllocationIndices(LocalExecutable *local_executable,
+                               std::vector<int64_t> *indices);
+
+}  // namespace xla
+
+#endif  // TENSORFLOW_COMPILER_JIT_XLA_LIB_RUNTIME_WORKSPACE_BYTES_H_
-- 
1.8.3.1

