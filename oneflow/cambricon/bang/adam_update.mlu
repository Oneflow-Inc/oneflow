/*
Copyright 2020 The OneFlow Authors. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
#include "oneflow/cambricon/bang/bang_kernels.h"

namespace oneflow {

static constexpr int32_t nram_limit = 1024;

__mlu_func__ void load_model_diff(float* dst, const float* src, int32_t size) {
  __memcpy_async(dst, src, size * sizeof(float), GDRAM2NRAM);
}

__mlu_func__ void load_model_diff(float* dst, const half* src, int32_t size) {
  __nram__ half temp[nram_limit];
  __memcpy_async(temp, src, size * sizeof(half), GDRAM2NRAM);
  __bang_half2float(dst, temp, size);
}

template<typename T, typename G>
__mlu_global__ void bang_adam_update_internal(
    int64_t n, T scale, float l1, float l2, float beta1, float beta2, float epsilon,
    float weight_decay, bool amsgrad, bool do_bias_correction, float learning_rate, float lr_scale,
    float bias_correction1, float bias_correction2, const float* learning_rate_ptr,
    const T* scale_by_ptr, const int64_t* skip_if, const float* bias_correction1_ptr,
    const float* bias_correction2_ptr, const G* model_diff, T* model, half* model_copy, T* m, T* v,
    T* max_v) {
  if (skip_if && *skip_if != 0) { return; }
  if (learning_rate_ptr) { learning_rate = *learning_rate_ptr; }
  if (scale_by_ptr) { scale *= *scale_by_ptr; }
  if (bias_correction1_ptr) { bias_correction1 = *bias_correction1_ptr; }
  if (bias_correction2_ptr) { bias_correction2 = *bias_correction2_ptr; }

  learning_rate *= lr_scale;
  T inv_bias_correction2 = 1.f / sqrt(bias_correction2);
  T step_size = learning_rate / bias_correction1;
  T lr_decay = learning_rate * weight_decay;

  int64_t step = (n + taskDim - 1) / taskDim;
  int64_t start = step * taskId;
  int64_t end = start + step;
  if (end > n) { end = n; }
  int64_t length = start < end ? end - start : 0;
  int64_t nram_rest = (length & (nram_limit - 1));  // length % nram_limit2
  int32_t nram_limit_bytes = nram_limit * sizeof(T);

  __nram__ T nram_model[nram_limit];
  __nram__ T nram_model_diff[nram_limit];
  __nram__ T nram_m[nram_limit];
  __nram__ T nram_v[nram_limit];
  __nram__ T nram_max_v[nram_limit];
  __nram__ T nram_temp0[nram_limit];
  __nram__ T nram_temp1[nram_limit];

  int64_t j = 0;
  for (; j < length - nram_limit + 1; j += nram_limit) {
    __memcpy_async(nram_model, model + start + j, nram_limit_bytes, GDRAM2NRAM);

    load_model_diff(nram_model_diff, model_diff + start + j, nram_limit);

    __memcpy_async(nram_m, m + start + j, nram_limit_bytes, GDRAM2NRAM);
    __memcpy_async(nram_v, v + start + j, nram_limit_bytes, GDRAM2NRAM);
    __sync_copy_dram_to_nram();

    __bang_mul_scalar(nram_model_diff, nram_model_diff, scale, nram_limit);
    __bang_ge_scalar(nram_temp0, nram_model, 0, nram_limit);
    __bang_le_scalar(nram_temp1, nram_model, 0, nram_limit);
    __bang_sub(nram_temp0, nram_temp0, nram_temp1, nram_limit);

    __bang_mul_scalar(nram_temp0, nram_temp0, l1, nram_limit);
    __bang_mul_scalar(nram_temp1, nram_model, l2, nram_limit);
    __bang_fusion(FUSION_FAA, nram_model_diff, nram_model_diff, nram_temp0, nram_temp1, nram_limit,
                  nram_limit);

    __bang_mul_scalar(nram_m, nram_m, beta1, nram_limit);
    __bang_mul_scalar(nram_v, nram_v, beta2, nram_limit);
    __bang_mul_scalar(nram_temp0, nram_model_diff, 1.0f - beta1, nram_limit);
    __bang_mul(nram_temp1, nram_model_diff, nram_model_diff, nram_limit);
    __bang_mul_scalar(nram_temp1, nram_temp1, 1.0f - beta2, nram_limit);

    __bang_add(nram_m, nram_m, nram_temp0, nram_limit);
    __bang_add(nram_v, nram_v, nram_temp1, nram_limit);

    __sync_compute();
    __memcpy_async(m + start + j, nram_m, nram_limit_bytes, NRAM2GDRAM);
    __memcpy_async(v + start + j, nram_v, nram_limit_bytes, NRAM2GDRAM);

    if (amsgrad) {
      __memcpy_async(nram_max_v, max_v + start + j, nram_limit_bytes, GDRAM2NRAM);
      __sync_copy_dram_to_nram();
      __bang_maxequal(nram_max_v, nram_max_v, nram_v, nram_limit);

      __sync_compute();
      __memcpy_async(max_v + start + j, nram_max_v, nram_limit_bytes, NRAM2GDRAM);
      __bang_sqrt(nram_temp0, nram_max_v, nram_limit);
    } else {
      __bang_sqrt(nram_temp0, nram_v, nram_limit);
    }
    __bang_mul_scalar(nram_temp0, nram_temp0, inv_bias_correction2, nram_limit);
    __bang_add_scalar(nram_temp0, nram_temp0, epsilon, nram_limit);

    // p = step_size * (m_val / denom)
    __bang_recip(nram_temp0, nram_temp0, nram_limit);
    __bang_mul(nram_temp0, nram_m, nram_temp0, nram_limit);
    __bang_mul_scalar(nram_temp0, nram_temp0, step_size, nram_limit);

    // q = learning_rate * weight_decay * model_val
    __bang_mul_scalar(nram_temp1, nram_model, lr_decay, nram_limit);

    // model_val - p - q
    __bang_fusion(FUSION_FSS, nram_model, nram_model, nram_temp0, nram_temp1, nram_limit,
                  nram_limit);

    __sync_compute();
    __memcpy_async(model + start + j, nram_model, nram_limit_bytes, NRAM2GDRAM);

    if (model_copy) {
      // TODO
    }
    __sync_copy_nram_to_dram();
  }

  if (nram_rest > 0) {
    int32_t nram_rest_bytes = nram_rest * sizeof(T);
    __memcpy_async(nram_model, model + start + j, nram_rest_bytes, GDRAM2NRAM);

    load_model_diff(nram_model_diff, model_diff + start + j, nram_rest);

    __memcpy_async(nram_m, m + start + j, nram_rest_bytes, GDRAM2NRAM);
    __memcpy_async(nram_v, v + start + j, nram_rest_bytes, GDRAM2NRAM);
    __sync_copy_dram_to_nram();

    __bang_mul_scalar(nram_model_diff, nram_model_diff, scale, nram_rest);
    __bang_ge_scalar(nram_temp0, nram_model, 0, nram_rest);
    __bang_le_scalar(nram_temp1, nram_model, 0, nram_rest);
    __bang_sub(nram_temp0, nram_temp0, nram_temp1, nram_rest);

    __bang_mul_scalar(nram_temp0, nram_temp0, l1, nram_rest);
    __bang_mul_scalar(nram_temp1, nram_model, l2, nram_rest);
    __bang_fusion(FUSION_FAA, nram_model_diff, nram_model_diff, nram_temp0, nram_temp1, nram_rest,
                  nram_rest);

    __bang_mul_scalar(nram_m, nram_m, beta1, nram_rest);
    __bang_mul_scalar(nram_v, nram_v, beta2, nram_rest);
    __bang_mul_scalar(nram_temp0, nram_model_diff, 1.0f - beta1, nram_rest);
    __bang_mul(nram_temp1, nram_model_diff, nram_model_diff, nram_rest);
    __bang_mul_scalar(nram_temp1, nram_temp1, 1.0f - beta2, nram_rest);

    __bang_add(nram_m, nram_m, nram_temp0, nram_rest);
    __bang_add(nram_v, nram_v, nram_temp1, nram_rest);

    __sync_compute();
    __memcpy_async(m + start + j, nram_m, nram_rest_bytes, NRAM2GDRAM);
    __memcpy_async(v + start + j, nram_v, nram_rest_bytes, NRAM2GDRAM);

    if (amsgrad) {
      __memcpy_async(nram_max_v, max_v + start + j, nram_rest_bytes, GDRAM2NRAM);
      __sync_copy_dram_to_nram();
      __bang_maxequal(nram_max_v, nram_max_v, nram_v, nram_rest);

      __sync_compute();
      __memcpy_async(max_v + start + j, nram_max_v, nram_rest_bytes, NRAM2GDRAM);
      __bang_sqrt(nram_temp0, nram_max_v, nram_rest);
    } else {
      __bang_sqrt(nram_temp0, nram_v, nram_rest);
    }
    __bang_mul_scalar(nram_temp0, nram_temp0, inv_bias_correction2, nram_rest);
    __bang_add_scalar(nram_temp0, nram_temp0, epsilon, nram_rest);

    // p = step_size * (m_val / denom)
    __bang_recip(nram_temp0, nram_temp0, nram_rest);
    __bang_mul(nram_temp0, nram_m, nram_temp0, nram_rest);
    __bang_mul_scalar(nram_temp0, nram_temp0, step_size, nram_rest);

    // q = learning_rate * weight_decay * model_val
    __bang_mul_scalar(nram_temp1, nram_model, lr_decay, nram_rest);

    // model_val - p - q
    __bang_fusion(FUSION_FSS, nram_model, nram_model, nram_temp0, nram_temp1, nram_rest, nram_rest);

    __sync_compute();
    __memcpy_async(model + start + j, nram_model, nram_rest_bytes, NRAM2GDRAM);

    if (model_copy) {
      // TODO
    }
    __sync_copy_nram_to_dram();
  }
}

template<typename T>
void bang_adam_update_kernel(BangHandle& handle, int64_t n, T scale, float l1, float l2,
                             float beta1, float beta2, float epsilon, float weight_decay,
                             bool amsgrad, bool do_bias_correction, float learning_rate,
                             float lr_scale, float bias_correction1, float bias_correction2,
                             const float* learning_rate_ptr, const T* scale_by_ptr,
                             const int64_t* skip_if, const float* bias_correction1_ptr,
                             const float* bias_correction2_ptr, const T* model_diff, T* model,
                             void* model_copy, T* m, T* v, T* max_v) {
  cnrtDim3_t dim = {handle.nclusters * handle.ncores_per_cluster, 1, 1};
  cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_UNION4;
  bang_adam_update_internal<<<dim, func_type, handle.queue>>>(
      n, scale, l1, l2, beta1, beta2, epsilon, weight_decay, amsgrad, do_bias_correction,
      learning_rate, lr_scale, bias_correction1, bias_correction2, learning_rate_ptr, scale_by_ptr,
      skip_if, bias_correction1_ptr, bias_correction2_ptr, model_diff, model,
      static_cast<half*>(model_copy), m, v, max_v);
}

template<typename T>
void bang_adam_update_half_kernel(BangHandle& handle, int64_t n, T scale, float l1, float l2,
                                  float beta1, float beta2, float epsilon, float weight_decay,
                                  bool amsgrad, bool do_bias_correction, float learning_rate,
                                  float lr_scale, float bias_correction1, float bias_correction2,
                                  const float* learning_rate_ptr, const T* scale_by_ptr,
                                  const int64_t* skip_if, const float* bias_correction1_ptr,
                                  const float* bias_correction2_ptr, const void* model_diff,
                                  T* model, void* model_copy, T* m, T* v, T* max_v) {
  cnrtDim3_t dim = {handle.nclusters * handle.ncores_per_cluster, 1, 1};
  cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_UNION4;
  bang_adam_update_internal<<<dim, func_type, handle.queue>>>(
      n, scale, l1, l2, beta1, beta2, epsilon, weight_decay, amsgrad, do_bias_correction,
      learning_rate, lr_scale, bias_correction1, bias_correction2, learning_rate_ptr, scale_by_ptr,
      skip_if, bias_correction1_ptr, bias_correction2_ptr, static_cast<const half*>(model_diff),
      model, static_cast<half*>(model_copy), m, v, max_v);
}

#define INSTANCE_BANG_ADAM_UPDATE_KERNEL(T)                                                      \
  template void bang_adam_update_kernel<T>(                                                      \
      BangHandle & handle, int64_t n, T scale, float l1, float l2, float beta1, float beta2,     \
      float epsilon, float weight_decay, bool amsgrad, bool do_bias_correction,                  \
      float learning_rate, float lr_scale, float bias_correction1, float bias_correction2,       \
      const float* learning_rate_ptr, const T* scale_by_ptr, const int64_t* skip_if,             \
      const float* bias_correction1_ptr, const float* bias_correction2_ptr, const T* model_diff, \
      T* model, void* model_copy, T* m, T* v, T* max_v);

#define INSTANCE_BANG_ADAM_UPDATE_HALF_KERNEL(T)                                             \
  template void bang_adam_update_half_kernel<T>(                                             \
      BangHandle & handle, int64_t n, T scale, float l1, float l2, float beta1, float beta2, \
      float epsilon, float weight_decay, bool amsgrad, bool do_bias_correction,              \
      float learning_rate, float lr_scale, float bias_correction1, float bias_correction2,   \
      const float* learning_rate_ptr, const T* scale_by_ptr, const int64_t* skip_if,         \
      const float* bias_correction1_ptr, const float* bias_correction2_ptr,                  \
      const void* model_diff, T* model, void* model_copy, T* m, T* v, T* max_v);

INSTANCE_BANG_ADAM_UPDATE_KERNEL(float)
INSTANCE_BANG_ADAM_UPDATE_HALF_KERNEL(float)

#undef INSTANCE_BANG_ADAM_UPDATE_KERNEL
#undef INSTANCE_BANG_ADAM_UPDATE_HALF_KERNEL

}  // namespace oneflow
