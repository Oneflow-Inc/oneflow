syntax = "proto2";

package caffe;


message BlobProto {
  required int64 shape = 1;
  enum DataType {
    UNDEFINED = 0;
    FLOAT = 1;  // float
    INT32 = 2;  // int
    BYTE = 3;  // BYTE, when deserialized, is going to be restored as uint8.
    STRING = 4;  // string
    // Less-commonly used data types.
    BOOL = 5;  // bool
    UINT8 = 6;  // uint8_t
    INT8 = 7;  // int8_t
    UINT16 = 8;  // uint16_t
    INT16 = 9;  // int16_t
    INT64 = 10;  // int64_t
    FLOAT16 = 12;  // caffe2::__f16, caffe2::float16
    DOUBLE = 13;  // double
  }
  optional DataType data_type = 2 [default = FLOAT];
  // For float
  repeated float float_data = 3 [packed = true];
  // For int32, uint8, int8, uint16, int16, bool, and float16
  // Note about float16: in storage we will basically convert float16 byte-wise
  // to unsigned short and then store them in the int32_data field.
  repeated int32 int32_data = 4 [packed = true];
  // For bytes
  optional bytes byte_data = 5;
  // For strings
  repeated bytes string_data = 6;
  // For double
  repeated double double_data = 9 [packed = true];
  // For int64
  repeated int64 int64_data = 10 [packed = true];
  optional string name = 7;
}

// Specifies the shape (dimensions) of a Blob.
message BlobShape {
  repeated int64 dim = 1 [packed = true];
}

message FillerParameter {
  // The filler type.
  optional string type = 1 [default = "constant"];
  optional float value = 2 [default = 0]; // the value in constant filler
  optional float min = 3 [default = 0]; // the min value in uniform filler
  optional float max = 4 [default = 1]; // the max value in uniform filler
  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler
  optional float std = 6 [default = 1]; // the std value in Gaussian filler
  // The expected number of non-zero output weights for a given input in
  // Gaussian filler -- the default -1 means don't perform sparsification.
  optional int32 sparse = 7 [default = -1];
}

message NetParameter {
  optional string name = 1; // consider giving the network a name
  // The shape of the input blobs.
  repeated BlobShape input_shape = 8;

  // Whether the network will force every layer to carry out backward operation.
  // If set False, then whether to carry out backward is determined
  // automatically according to the net structure and learning rates.
  optional bool force_backward = 5 [default = false];
  // The current "state" of the network, including the phase, level, and stage.
  // Some layers may be included/excluded depending on this state and the states
  // specified in the layers' include and exclude fields.
  optional NetState state = 6;

  // Print debugging information about results while running Net::Forward,
  // Net::Backward, and Net::Update.
  optional bool debug_info = 7 [default = false];

  // The layers that make up the net.  Each of their configurations, including
  // connectivity and behavior, is specified as a LayerParameter.
  repeated LayerProto layer = 100;  // ID 100 so layers are printed last.
}

message DeviceSet {
  repeated int32 device_id = 1; // the devices list on each worker machine
}
message Machine {
  required string name = 1;  // Could be computer name or IP address
  required string port = 2;
  optional DeviceSet device_set = 3;  // List the devices' physical IDs
}
message Resource {
  repeated Machine machine = 1;
}

message LayerSet {
  repeated string name = 1;
}
message DeviceGroup {
  required int32 begin = 1;  // Specify the range of devices' logical IDs
  required int32 end = 2;
}
message MachineGroup {
  required int32 begin = 1;  // Specify the range of machines' logical IDs
  required int32 end = 2;
}

enum ParallelPolicy {
   kUnknownParallel = 0;
   kNaiveParallelOnSingleDevice = 1;     // Single device
   kDataParallelOnMultipleDevices = 2;   // Multiple devices, data-parallelization
   kModelParallelOnMultipleDevices = 3;  // Multiple devices, model-parallelization
   kNaiveParallelOnSingleMachine = 4;    // Single host
   kDataParallelOnMultipleMachines = 5;  // Multiple hosts, data-parallelization
   kModelParallelOnMultipleMachines = 6; // Multiple hosts, model-parallelization
}
message PlacementGroup {
  required string name = 1;
  required LayerSet layer_set = 2;
  optional DeviceGroup device_group = 3;  // No need to set devices for data group
  optional MachineGroup machine_group = 4;
  optional ParallelPolicy parallel_policy = 5 [default = kUnknownParallel];
}

message Strategy {
  repeated PlacementGroup placement_group = 1;
}

// NOTE
// Update the next available ID when you add a new SolverProto field.
//
// SolverProto next available ID: 66 (last added: num_batch_per_sync)
message SolverProto {
  //////////////////////////////////////////////////////////////////////////////
  // Specifying the train and test networks
  //
  // Exactly one train net must be specified using the following field:
  //     train_net
  // One or more test nets may be specified using any of the following fields:
  //     test_net
  // A test_iter must be specified for each test_net.
  // A test_level and/or a test_stage may also be specified for each test_net.
  //////////////////////////////////////////////////////////////////////////////

  required string train_net = 1; // Proto filename for the train net.
  repeated string test_net = 2; // Proto filenames for the test nets.
  required string resource = 3;
  required string strategy = 4;

  // The states for the train/test nets. Must be unspecified or
  // specified once per net.
  //
  // By default, all states will have solver = true;
  // train_state will have phase = TRAIN,
  // and all test_state's will have phase = TEST.
  // Other defaults are set according to the NetState defaults.
  optional NetState train_state = 26;
  repeated NetState test_state = 27;

  // The number of iterations for each test net.
  repeated int32 test_iter = 28;

  // The number of iterations between two testing phases.
  optional int32 test_interval = 29 [default = 0];
  optional bool test_compute_loss = 30 [default = false];
  // If true, run an initial test pass before the first iteration,
  // ensuring memory availability and printing the starting value of the loss.
  optional bool test_initialization = 32 [default = true];
  optional float base_lr = 33; // The base learning rate
  // the number of iterations between displaying info. If display = 0, no info
  // will be displayed.
  optional int32 display = 34;
  // Display the loss averaged over the last average_loss iterations
  optional int32 average_loss = 35 [default = 1];
  optional int32 max_iter = 36; // the maximum number of iterations
  optional string lr_policy = 37; // The learning rate decay policy.
  optional float gamma = 39; // The parameter to compute the learning rate.
  optional float power = 40; // The parameter to compute the learning rate.
  optional float momentum = 41; // The momentum value.
  optional float weight_decay = 42; // The weight decay.
  // regularization types supported: L1 and L2
  // controlled by weight_decay
  optional string regularization_type = 43 [default = "L2"];
  // the stepsize for learning rate policy "step"
  optional int32 stepsize = 44;
  // the stepsize for learning rate policy "multistep"
  repeated int32 stepvalue = 45;

  // Set clip_gradients to >= 0 to clip parameter gradients to that L2 norm,
  // whenever their actual L2 norm is larger.
  optional float clip_gradients = 46 [default = -1];

  optional int32 snapshot = 47 [default = 0]; // The snapshot interval
  optional string snapshot_prefix = 48; // The prefix for the snapshot.
  // whether to snapshot diff in the results or not. Snapshotting diff will help
  // debugging but the final protocol buffer size will be much larger.
  optional bool snapshot_diff = 49 [default = false];
  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.
  enum SolverMode {
    CPU = 0;
    GPU = 1;
  }
  // If non-negative, the seed with which the Solver will initialize the Caffe
  // random number generator -- useful for reproducible results. Otherwise,
  // (and by default) initialize using a seed derived from the system clock.
  optional int64 random_seed = 52 [default = -1];
  required int32 machine_id = 56 [default = 0];

  // Solver type
  enum SolverType {
    SGD = 0;
    NESTEROV = 1;
    ADAGRAD = 2;
  }
  optional SolverType solver_type = 57 [default = SGD];
  // numerical stability for AdaGrad
  optional float delta = 59 [default = 1e-8];

  // If true, print information about the state of the net that may help with
  // debugging learning problems.
  optional bool debug_info = 60 [default = false];

  // If false, don't save a snapshot after training finishes.
  optional bool snapshot_after_train = 62 [default = true];

  // Number of data_param copies to maintain
  optional int32 num_data_param_copy = 63 [default = 1];

  // Number of model_param copied to maintain
  optional int32 num_model_param_copy = 64 [default = 1];

  // Number of batches of data to perform one sync
  optional int32 num_batch_per_sync = 65 [default = 1];
}

// A message that stores the solver snapshots
message SolverState {
  optional int32 iter = 1; // The current iteration
  optional string learned_net = 2; // The file that stores the learned net.
  // repeated BlobProto history = 3; // The history for sgd solvers
  optional int32 current_step = 4 [default = 0]; // The current step for learning rate
}

enum Phase {
   TRAIN = 0;
   TEST = 1;
}

message NetState {
  optional Phase phase = 1 [default = TEST];
  optional int32 level = 2 [default = 0];
  repeated string stage = 3;
}

// Specifies training parameters (multipliers on global learning constants,
// and the name and other settings used for weight sharing).
message ParamSpec {
  // The names of the parameter blobs -- useful for sharing parameters among
  // layers, but never required otherwise.  To share a parameter between two
  // layers, give it a (non-empty) name.
  optional string name = 1;

  // Whether to require shared weights to have the same shape, or just the same
  // count -- defaults to STRICT if unspecified.
  optional DimCheckMode share_mode = 2;
  enum DimCheckMode {
    // STRICT (default) requires that num, channels, height, width each match.
    STRICT = 0;
    // PERMISSIVE requires only the count (num*channels*height*width) to match.
    PERMISSIVE = 1;
  }

  // The multiplier on the global learning rate for this parameter.
  optional float lr_mult = 3 [default = 1.0];

  // The multiplier on the global weight decay for this parameter.
  optional float decay_mult = 4 [default = 1.0];
}

// NOTE
// Update the next available ID when you add a new LayerParameter field.
//
// LayerParameter next available layer-specific ID: 135 (last added: generate_gradient_proto)
message LayerProto {
  optional string name = 1; // the layer name
  optional string type = 2; // the layer type

  // NOTE(jiyuan): Be cautious to the naming rules, different from the original
  // caffe.proto. The variable name should be exactly equal to the lower case of
  // the type name. For example, InnerProductProto->innerproduct_proto,
  // instead of InnerProductProto->inner_product_proto in original caffe.proto
  optional ConvolutionProto convolution_proto = 106;
  optional LoaderProto loader_proto = 107;
  optional InnerProductProto innerproduct_proto = 117;
  optional PoolingProto pooling_proto = 121;
  optional ReLUProto relu_proto = 123;
  optional LRNProto lrn_proto = 118;
  optional SigmoidProto sigmoid_proto = 124;
  optional SoftmaxProto softmax_proto = 125;
  optional MultinomialLogisticLossProto multinomiallogisticloss_proto = 130;
  optional SplitProto split_proto = 131;
  optional ConcatProto concat_proto = 132;
  optional BatchNormProto batchnorm_proto = 135;
  optional ModelUpdateProto modelupdate_proto = 136;
  optional NullUpdateProto nullupdate_proto = 137;
  optional StoreProto store_proto = 139;
  optional LoadPartialModelProto loadpartialmodel_proto = 140;
  optional PlaceholderProto placeholder_proto = 141;
}

message BatchNormProto {
  // If false, accumulate global mean/variance values via a moving average. If
  // true, use those accumulated values instead of computing mean/variance
  // across the batch.
  optional bool use_global_stats = 1;
  // How much does the moving average decay each iteration?
  optional float moving_average_fraction = 2 [default = 0.999];
  // Small value to add to the variance estimate so that we don't divide by
  // zero.
  optional float eps = 3 [default = 1e-5];
}

message MultinomialLogisticLossProto {
  required string data = 120;
  required string label = 121;
  required string loss = 122;
}

enum CopyType {
  ForwardH2D = 0;  // in_copy
  ForwardD2H = 1;  // out_copy
  ForwardD2D = 2;  // device_to_device
}

message CopyProto {
  required uint32 num = 1;
  required CopyType copy_type = 2;
  repeated string in = 3;
  repeated string out = 4;
}
enum BoxingOp {
  CONCAT = 0;
  ADD = 1;
  COPY = 2;
  SPLIT = 3;
}
message BoxingProto {
  required uint32 in_num = 1;
  repeated string in = 2;
  required BoxingOp in_op = 3 [default = CONCAT];
  required int32 in_axis = 4 [default = 0];
  required BoxingOp backward_in_op = 5 [default = SPLIT];

  required uint32 out_num = 110;
  repeated string out = 12;
  required BoxingOp out_op = 13 [default = COPY];
  required int32 out_axis = 14 [default = 0];
  required BoxingOp backward_out_op = 15 [default = ADD];
}

message ModelUpdateProto {
  required string gradient = 1;
  required string old_weight = 2;
  required string weight = 3;
}
// For testing purpose, no need to truly add gradient to weight
message NullUpdateProto {
  required string weight = 1;
}
message NetProto {
  required string in_envelope = 1;
  required uint32 in_num = 2;
  repeated string in = 3;
  required string out_envelope = 4;
  required uint32 out_num = 5;
  repeated string out = 6;
  required bool forward_is_sender = 7 [default = true];
}
message SplitProto {
  required uint32 out_num = 1;
  required string in = 2;
  repeated string out = 3;
}
message ConcatProto {
  required uint32 in_num = 1;
  repeated string in = 2;
  required string out = 3;
  // The axis along which to concatenate -- may be negative to index from the
  // end (e.g., -1 for the last axis).  Other axes must have the
  // same dimension for all the bottom blobs.
  // By default, ConcatLayer concatenates blobs along the "channels" axis (1).
  required int32 axis = 4 [default = 1];

}
// Message that stores parameters used by ConvolutionLayer
message ConvolutionProto {
  optional uint32 num_output = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  // Pad, kernel size, and stride are all given as a single value for equal
  // dimensions in height and width or as Y, X pairs.
  optional uint32 pad = 3 [default = 0]; // The padding size (equal in Y, X)
  optional uint32 pad_h = 9 [default = 0]; // The padding height
  optional uint32 pad_w = 10 [default = 0]; // The padding width
  optional uint32 kernel_size = 4; // The kernel size (square)
  optional uint32 kernel_h = 11; // The kernel height
  optional uint32 kernel_w = 12; // The kernel width
  optional uint32 group = 5 [default = 1]; // The group size for group conv
  optional uint32 stride = 6 [default = 1]; // The stride (equal in Y, X)
  optional uint32 stride_h = 13; // The stride height
  optional uint32 stride_w = 14; // The stride width
  optional FillerParameter weight_filler = 7; // The filler for the weight
  optional FillerParameter bias_filler = 8; // The filler for the bias
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 15 [default = DEFAULT];
  // Specifies training parameters (multipliers on global learning constants,
  // and the name and other settings used for weight sharing).
  repeated ParamSpec param = 100;
  required string in = 120;
  required string out = 121;
}

// Message that stores parameters used by DataLayer
message LoaderProto {
  // Specify the data source.
  optional string source = 1;
  // Specify the piece size.
  optional uint32 piece_size = 4;
  // The rand_skip variable is for the data layer to skip a few data points
  // to avoid all asynchronous sgd clients to start at the same point. The skip
  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not
  // be larger than the number of keys in the database.
  // optional uint32 rand_skip = 7 [default = 0];
  // optional DB backend = 8 [default = LEVELDB];
  // Force the encoded image to have 3 color channels
  // optional bool force_encoded_color = 9 [default = false];

  // Parameters for data pre-processing.
  // optional TransformationProto transform_proto = 100;
  required string data = 120;
  required string label = 121;

  // The batch size is specified elsewhere
  required uint32 channel = 122;
  required uint32 height = 123;
  required uint32 width = 124;
}

// Message that stores parameters used by InnerProductLayer
message InnerProductProto {
  optional uint32 num_output = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  optional FillerParameter weight_filler = 3; // The filler for the weight
  optional FillerParameter bias_filler = 4; // The filler for the bias

  // The first axis to be lumped into a single inner product computation;
  // all preceding axes are retained in the output.
  // May be negative to index from the end (e.g., -1 for the last axis).
  optional int32 axis = 5 [default = 1];
  // Specifies training parameters (multipliers on global learning constants,
  // and the name and other settings used for weight sharing).
  repeated ParamSpec param = 100;
  required string in = 120;
  required string out = 121;
}

// Message that stores parameters used by PoolingLayer
message PoolingProto {
  enum PoolMethod {
    MAX = 0;
    AVE = 1;
    STOCHASTIC = 2;
  }
  optional PoolMethod pool = 1 [default = MAX]; // The pooling method
  // Pad, kernel size, and stride are all given as a single value for equal
  // dimensions in height and width or as Y, X pairs.
  optional uint32 pad = 4 [default = 0]; // The padding size (equal in Y, X)
  optional uint32 pad_h = 9 [default = 0]; // The padding height
  optional uint32 pad_w = 10 [default = 0]; // The padding width
  optional uint32 kernel_size = 2; // The kernel size (square)
  optional uint32 kernel_h = 5; // The kernel height
  optional uint32 kernel_w = 6; // The kernel width
  optional uint32 stride = 3 [default = 1]; // The stride (equal in Y, X)
  optional uint32 stride_h = 7; // The stride height
  optional uint32 stride_w = 8; // The stride width
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 11 [default = DEFAULT];
  // If global_pooling then it will pool over the size of the bottom by doing
  // kernel_h = bottom->height and kernel_w = bottom->width
  optional bool global_pooling = 12 [default = false];
  required string in = 120;
  required string out = 121;
}

// Message that stores parameters used by ReLULayer
message ReLUProto {
  // Allow non-zero slope for negative inputs to speed up optimization
  // Described in:
  // Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013). Rectifier nonlinearities
  // improve neural network acoustic models. In ICML Workshop on Deep Learning
  // for Audio, Speech, and Language Processing.
  optional float negative_slope = 1 [default = 0];
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 2 [default = DEFAULT];
  required string in = 120;
  required string out = 121;
}

// Message that stores parameters used by LRNLayer
message LRNProto {
  optional uint32 local_size = 1 [default = 5];
  optional float alpha = 2 [default = 1.0];
  optional float beta = 3 [default = 0.75];
  enum NormRegion {
    ACROSS_CHANNELS = 0;
    WITHIN_CHANNEL = 1;
  }
  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];
  optional float k = 5 [default = 1.0];
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 6 [default = DEFAULT];
  required string in = 120;
  required string out = 121;
}

// Message that stores parameters used by SigmoidLayer
message SigmoidProto {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];
  required string in = 120;
  required string out = 121;
}

// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer
message SoftmaxProto {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 1 [default = DEFAULT];

  // The axis along which to perform the softmax -- may be negative to index
  // from the end (e.g., -1 for the last axis).
  // Any other axes will be evaluated as independent softmaxes.
  optional int32 axis = 2 [default = 1];
  required string in = 120;
  required string out = 121;
}

message StoreProto {
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }

  optional Engine engine = 1 [default = DEFAULT];
  required string in = 120;
  required string out = 121;
  required bool stop = 122 [default = false];

  repeated string store_layer_names = 124;
  repeated int64 store_layer_shapes = 125;
  repeated int64 layer_seek_pos = 126;
}

message PlaceholderProto {
  required string in = 100;
  required string out = 101;
}

// Initial proto for LoadPartialModel Layer
message LoadPartialModelProto {
  required string out = 1;
  repeated string load_layer_names = 124;
  repeated int64 load_layer_shapes = 125;
}