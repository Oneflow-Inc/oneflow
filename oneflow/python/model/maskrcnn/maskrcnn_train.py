# must import get cfg before importing oneflow
from config import get_default_cfgs

import os
import numpy as np
import pandas as pd
import argparse
import time
import statistics
import oneflow as flow

from datetime import datetime
from backbone import Backbone
from rpn import RPNHead, RPNLoss, RPNProposal
from box_head import BoxHead
from mask_head import MaskHead
from data_load import make_data_loader
from blob_watcher import save_blob_watched, blob_watched, diff_blob_watched

parser = argparse.ArgumentParser()
parser.add_argument(
    "-c", "--config_file", default=None, type=str, help="yaml config file"
)
parser.add_argument("-bz", "--batch_size", type=int, default=1, required=False)
parser.add_argument(
    "-load", "--model_load_dir", type=str, default="", required=False
)
parser.add_argument(
    "-g", "--gpu_num_per_node", type=int, default=1, required=False
)
parser.add_argument(
    "-d",
    "--debug",
    type=bool,
    default=True,
    required=False,
    help="debug with random data generated by numpy",
)
parser.add_argument(
    "-rpn", "--rpn_only", default=False, action="store_true", required=False
)
parser.add_argument(
    "-mp",
    "--mock_dataset_path",
    type=str,
    default="/tmp/shared_with_zwx/mock_data_600x1000_b2.pkl",
    required=False,
)
parser.add_argument(
    "-cp", "--ctrl_port", type=int, default=19765, required=False
)
parser.add_argument(
    "-save",
    "--model_save_dir",
    type=str,
    default="./model_save-{}".format(
        str(datetime.now().strftime("%Y-%m-%d--%H-%M-%S"))
    ),
    required=False,
)
parser.add_argument(
    "-save_every_n_batch",
    "--model_save_every_n_batch",
    type=int,
    default=0,
    required=False,
)
parser.add_argument(
    "-save_loss_npy_every_n_batch",
    "--save_loss_npy_every_n_batch",
    type=int,
    default=0,
    required=False,
)
parser.add_argument(
    "-v", "--verbose", default=False, action="store_true", required=False
)
parser.add_argument("-i", "--iter_num", type=int, default=10, required=False)
parser.add_argument(
    "-lr", "--primary_lr", type=float, default=0.0002, required=False
)
parser.add_argument(
    "-slr", "--secondary_lr", type=float, default=0.0004, required=False
)
parser.add_argument(
    "-fake", "--fake_image_path", type=str, default="", required=False
)
parser.add_argument(
    "-anno",
    "--annotation_file",
    type=str,
    default="instances_val2017.json",
    required=False,
)
parser.add_argument(
    "-img", "--image_dir", type=str, default="val2017", required=False
)
parser.add_argument(
    "-j", "--jupyter", default=False, action="store_true", required=False
)
parser.add_argument(
    "-ds",
    "--dataset_shuffle",
    default=False,
    action="store_true",
    required=False,
)
parser.add_argument(
    "-ss",
    "--sample_shuffle",
    default=False,
    action="store_true",
    required=False,
)
parser.add_argument(
    "-flip",
    "--random_flip_image",
    default=False,
    action="store_true",
    required=False,
)
parser.add_argument(
    "-pr",
    "--print_loss_each_rank",
    default=False,
    action="store_true",
    required=False,
)

terminal_args = parser.parse_args()


def MakeWatcherCallback(prompt):
    def Callback(blob, blob_def):
        if prompt == "forward":
            return
        print(
            "%s, lbn: %s, min: %s, max: %s"
            % (prompt, blob_def.logical_blob_name, blob.min(), blob.max())
        )

    return Callback


def maskrcnn_train(cfg, image, image_size, gt_bbox, gt_segm, gt_label):
    """Mask-RCNN
    Args:
        image: (N, H, W, C)
        image_size: (N, 2)
        gt_bbox: (N, M, 4), num_lod_lvl == 2
        gt_segm: (N, M, 28, 28), num_lod_lvl == 2
        gt_label: (N, M), num_lod_lvl == 2
    """
    assert image.shape[3] == 3
    assert gt_bbox.num_of_lod_levels == 2
    assert gt_segm.num_of_lod_levels == 2
    assert gt_label.num_of_lod_levels == 2

    if terminal_args.verbose:
        print(cfg)

    backbone = Backbone(cfg)
    rpn_head = RPNHead(cfg)
    rpn_loss = RPNLoss(cfg)
    rpn_proposal = RPNProposal(cfg)
    box_head = BoxHead(cfg)
    mask_head = MaskHead(cfg)

    image_size_list = [
        flow.squeeze(
            flow.local_gather(image_size, flow.constant(i, dtype=flow.int32)),
            [0],
            name="image{}_size".format(i),
        )
        for i in range(image_size.shape[0])
    ]

    gt_bbox_list = flow.piece_slice(
        gt_bbox, gt_bbox.shape[0], name="gt_bbox_per_img"
    )

    gt_label_list = flow.piece_slice(
        gt_label, gt_label.shape[0], name="gt_label_per_img"
    )

    gt_segm_list = flow.piece_slice(
        gt_segm, gt_segm.shape[0], name="gt_segm_per_img"
    )

    anchors = [
        flow.detection.anchor_generate(
            images=image,
            feature_map_stride=cfg.DECODER.FEATURE_MAP_STRIDE * pow(2, i),
            aspect_ratios=cfg.DECODER.ASPECT_RATIOS,
            anchor_scales=cfg.DECODER.ANCHOR_SCALES * pow(2, i),
        )
        for i in range(cfg.DECODER.FPN_LAYERS)
    ]

    # Backbone
    # CHECK_POINT: fpn features
    # with flow.watch_scope(
    #     blob_watcher=blob_watched, diff_blob_watcher=diff_blob_watched
    # ):
    features = backbone.build(flow.transpose(image, perm=[0, 3, 1, 2]))

    # RPN
    # with flow.watch_scope(
    #     blob_watcher=blob_watched, diff_blob_watcher=diff_blob_watched
    # ):
    cls_logit_list, bbox_pred_list = rpn_head.build(features)
    rpn_bbox_loss, rpn_objectness_loss = rpn_loss.build(
        anchors, image_size_list, gt_bbox_list, bbox_pred_list, cls_logit_list
    )

    if terminal_args.rpn_only:
        return rpn_bbox_loss, rpn_objectness_loss

    # with flow.watch_scope(blob_watched):
    proposals = rpn_proposal.build(
        anchors, cls_logit_list, bbox_pred_list, image_size_list, gt_bbox_list
    )

    # with flow.watch_scope(
    #     blob_watcher=blob_watched, diff_blob_watcher=diff_blob_watched
    # ), flow.watch_scope(
    #     blob_watcher=MakeWatcherCallback("forward"),
    #     diff_blob_watcher=MakeWatcherCallback("backward"),
    # ):
    # Box Head
    box_loss, cls_loss, pos_proposal_list, pos_gt_indices_list, total_pos_inds_elem_cnt = box_head.build_train(
        proposals, gt_bbox_list, gt_label_list, features
    )

    # Mask Head
    mask_loss = mask_head.build_train(
        pos_proposal_list,
        pos_gt_indices_list,
        gt_segm_list,
        gt_label_list,
        features,
    )

    return {
        "loss_rpn_box_reg": rpn_bbox_loss,
        "loss_objectness": rpn_objectness_loss,
        "loss_box_reg": box_loss,
        "loss_classifier": cls_loss,
        "loss_mask": mask_loss,
        "total_pos_inds_elem_cnt": total_pos_inds_elem_cnt,
    }


def init_config():
    config = get_default_cfgs()
    if terminal_args.config_file is not None:
        config.merge_from_file(terminal_args.config_file)
        print("merged config from {}".format(terminal_args.config_file))

    if "gpu_num_per_node" in terminal_args:
        config.NUM_GPUS = terminal_args.gpu_num_per_node

    assert terminal_args.batch_size % terminal_args.gpu_num_per_node == 0
    config.TRAINING_CONF.IMG_PER_GPU = int(
        terminal_args.batch_size / terminal_args.gpu_num_per_node
    )

    config.INFERENCE.PROPOSAL_RANDOM_SAMPLE = terminal_args.sample_shuffle

    config.freeze()

    if terminal_args.verbose:
        print(config)

    flow.config.cudnn_buf_limit_mbyte(1280)
    flow.config.cudnn_conv_heuristic_search_algo(True)
    flow.config.cudnn_conv_use_deterministic_algo_only(False)
    flow.config.train.primary_lr(config.TRAINING_CONF.PRIMARY_LR)
    flow.config.train.secondary_lr(config.TRAINING_CONF.SECONDARY_LR)
    flow.config.train.weight_l2(config.TRAINING_CONF.WEIGHT_L2)
    flow.config.train.bias_l2(config.TRAINING_CONF.BIAS_L2)

    if config.TRAINING_CONF.USE_MOMENTUM_SGD:
        optimizer = dict(
            momentum_conf={"beta": config.TRAINING_CONF.MOMENTUM_BETA}
        )
    else:
        optimizer = dict(naive_conf={})

    if not config.TRAINING_CONF.DISABLE_LR_DECAY:
        optimizer.update(
            dict(
                learning_rate_decay=dict(
                    piecewise_constant_conf={
                        "boundaries": config.TRAINING_CONF.LR_DECAY_BOUNDARIES,
                        "values": config.TRAINING_CONF.LR_DECAY_VALUES,
                    }
                )
            )
        )

    if not config.TRAINING_CONF.DISABLE_WARMUP:
        if config.TRAINING_CONF.WARMUP_METHOD == "linear":
            optimizer.update(
                {
                    "warmup_conf": {
                        "linear_conf": {
                            "warmup_batches": config.TRAINING_CONF.WARMUP_BATCHES,
                            "start_multiplier": config.TRAINING_CONF.WARMUP_FACTOR,
                        }
                    }
                }
            )
        elif config.TRAINING_CONF.WARMUP_METHOD == "constant":
            raise NotImplementedError
        else:
            raise ValueError

    flow.config.train.model_update_conf(optimizer)

    return config


def save_model(check_point, i):
    if not os.path.exists(terminal_args.model_save_dir):
        os.makedirs(terminal_args.model_save_dir)
    model_dst = os.path.join(terminal_args.model_save_dir, "iter-" + str(i))
    print("saving models to {}".format(model_dst))
    check_point.save(model_dst)


def train_net(config, image=None):
    flow.config.default_initialize_with_snapshot_path(
        terminal_args.model_load_dir
    )

    data_loader = make_data_loader(
        batch_size=terminal_args.batch_size,
        batch_cache_size=3,
        annotation_file=terminal_args.annotation_file,
        image_dir=terminal_args.image_dir,
        shuffle=terminal_args.dataset_shuffle,
        random_flip_image=terminal_args.random_flip_image,
    )

    loss_name_tup = (
        "loss_rpn_box_reg",
        "loss_objectness",
        "loss_box_reg",
        "loss_classifier",
        "loss_mask",
    )

    if config.NUM_GPUS > 1:
        distribute_train_func = flow.experimental.mirror_execute(
            config.NUM_GPUS, 1
        )(maskrcnn_train)
        outputs = distribute_train_func(
            config,
            flow.identity(image) if image else data_loader("image"),
            data_loader("image_size"),
            data_loader("gt_bbox"),
            data_loader("gt_segm"),
            data_loader("gt_labels"),
        )
        for outputs_per_rank in outputs:
            for k, v in outputs_per_rank.items():
                if k in loss_name_tup:
                    flow.losses.add_loss(v)

    else:
        outputs = maskrcnn_train(
            config,
            image or data_loader("image"),
            data_loader("image_size"),
            data_loader("gt_bbox"),
            data_loader("gt_segm"),
            data_loader("gt_labels"),
        )
        for k, v in outputs.items():
            if k in loss_name_tup:
                flow.losses.add_loss(v)

    return outputs


def init_train_func(fake_image):
    if fake_image:

        @flow.function
        def train(
            image_blob=flow.input_blob_def(
                shape=(2, 800, 1344, 3), dtype=flow.float32, is_dynamic=True
            )
        ):
            config = init_config()
            return train_net(config, image_blob)

        return train

    else:

        @flow.function
        def train():
            config = init_config()
            return train_net(config)

        return train


def print_metric_title():
    if terminal_args.print_loss_each_rank:
        print(
            "{:<8} {:<8} {:<16} {:<16} {:<16} {:<16} {:<16} {:<16}".format(
                "iter",
                "rank",
                "elapsed_time",
                "loss_rpn_box_reg",
                "loss_objectness",
                "loss_box_reg",
                "loss_classifier",
                "loss_mask",
            )
        )
    else:
        print(
            "{:<8} {:<16} {:<16} {:<16} {:<16} {:<16} {:<16}".format(
                "iter",
                "elapsed_time",
                "loss_rpn_box_reg",
                "loss_objectness",
                "loss_box_reg",
                "loss_classifier",
                "loss_mask",
            )
        )


def update_metrics(metrics, iter, elapsed_time, outputs):
    elapsed_time_str = "{:.6f}".format(elapsed_time)
    if terminal_args.print_loss_each_rank:
        for rank, output_per_rank in enumerate(outputs):
            fmt = "{:<8} {:<8} {:<16} {:<16.8f} {:<16.8f} {:<16.8f} {:<16.8f} {:<16.8f} {:<8}"
            loss_rpn_box_reg = output_per_rank["loss_rpn_box_reg"]
            loss_objectness = output_per_rank["loss_objectness"]
            loss_box_reg = output_per_rank["loss_box_reg"]
            loss_classifier = output_per_rank["loss_classifier"]
            loss_mask = output_per_rank["loss_mask"]
            total_pos_inds_elem_cnt = output_per_rank["total_pos_inds_elem_cnt"]
            print(
                fmt.format(
                    iter,
                    rank,
                    elapsed_time_str if rank == 0 else "",
                    loss_rpn_box_reg.mean(),
                    loss_objectness.mean(),
                    loss_box_reg.mean(),
                    loss_classifier.mean(),
                    loss_mask.mean(),
                    int(total_pos_inds_elem_cnt.item()),
                )
            )
    else:

        def reduce_loss(losses):
            loss_list_of_ranks = [loss.mean() for loss in losses]
            return sum(loss_list_of_ranks) / len(loss_list_of_ranks)

        loss_rpn_box_reg = reduce_loss(
            [output["loss_rpn_box_reg"] for output in outputs]
        )
        loss_objectness = reduce_loss(
            [output["loss_objectness"] for output in outputs]
        )
        loss_box_reg = reduce_loss(
            [output["loss_box_reg"] for output in outputs]
        )
        loss_classifier = reduce_loss(
            [output["loss_classifier"] for output in outputs]
        )
        loss_mask = reduce_loss([output["loss_mask"] for output in outputs])
        total_pos_inds_elem_cnt = sum(
            [output["total_pos_inds_elem_cnt"] for output in outputs]
        )

        print(
            "{:<8} {:<16} {:<16.8f} {:<16.8f} {:<16.8f} {:<16.8f} {:<16.8f} {:<8}".format(
                iter,
                elapsed_time_str,
                loss_rpn_box_reg,
                loss_objectness,
                loss_box_reg,
                loss_classifier,
                loss_mask,
                int(total_pos_inds_elem_cnt),
            )
        )

        df = pd.DataFrame(
            [
                {"iter": iter, "legend": "elapsed_time", "value": elapsed_time},
                {
                    "iter": iter,
                    "legend": "loss_rpn_box_reg",
                    "value": loss_rpn_box_reg,
                },
                {
                    "iter": iter,
                    "legend": "loss_objectness",
                    "value": loss_objectness,
                },
                {"iter": iter, "legend": "loss_box_reg", "value": loss_box_reg},
                {
                    "iter": iter,
                    "legend": "loss_classifier",
                    "value": loss_classifier,
                },
                {"iter": iter, "legend": "loss_mask", "value": loss_mask},
                {
                    "iter": iter,
                    "legend": "total_pos_inds_elem_cnt",
                    "value": total_pos_inds_elem_cnt,
                },
            ]
        )
        metrics = pd.concat([metrics, df], axis=0)

    if terminal_args.save_loss_npy_every_n_batch > 0:
        if (
            (iter + 1) % terminal_args.save_loss_npy_every_n_batch == 0
            or iter + 1 == terminal_args.iter_num
        ):
            npy_file_name = "loss-{}-batch_size-{}-gpu-{}-image_dir-{}-{}.csv".format(
                iter,
                terminal_args.batch_size,
                terminal_args.gpu_num_per_node,
                terminal_args.image_dir,
                str(datetime.now().strftime("%Y-%m-%d--%H-%M-%S")),
            )
            metrics.to_csv(npy_file_name, index=False)
            print("saved: {}".format(npy_file_name))

    return metrics


def run():
    flow.env.ctrl_port(terminal_args.ctrl_port)
    flow.config.enable_inplace(False)
    flow.config.gpu_device_num(terminal_args.gpu_num_per_node)
    flow.config.default_data_type(flow.float)

    fake_image_list = []
    if terminal_args.fake_image_path:
        file_list = os.listdir(terminal_args.fake_image_path)
        fake_image_list = [
            np.load(os.path.join(terminal_args.fake_image_path, f))
            for f in file_list
        ]

    # Get mrcn train function
    train_func = init_train_func(len(fake_image_list) > 0)

    # model init
    if not terminal_args.model_load_dir:
        check_point = flow.train.CheckPoint()
        check_point.init()
        if terminal_args.model_save_every_n_batch > 0:
            save_model(check_point, 0)
    else:
        check_point = flow.train.CheckPoint()
        # check_point.load(terminal_args.model_load_dir)
        check_point.init()

    start_time = time.time()
    elapsed_times = []
    metrics = pd.DataFrame()
    print_metric_title()
    for i in range(terminal_args.iter_num):
        if i < len(fake_image_list):
            outputs = train_func(fake_image_list[i]).get()
        else:
            outputs = train_func().get()

        now_time = time.time()
        elapsed_time = now_time - start_time
        elapsed_times.append(elapsed_time)
        start_time = now_time

        save_blob_watched(i)

        if terminal_args.model_save_every_n_batch > 0:
            if (
                (i + 1) % terminal_args.model_save_every_n_batch == 0
                or i + 1 == terminal_args.iter_num
            ):
                save_model(check_point, i + 1)

        metrics = update_metrics(metrics, i, elapsed_time, outputs)

    print("median of elapsed time per batch:", statistics.median(elapsed_times))


if __name__ == "__main__":
    run()
