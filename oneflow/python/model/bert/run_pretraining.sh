python run_pretraining.py \
  --gpu_num_per_node=1 \
  --node_num=1 \
  --node_list='192.168.1.15,192.168.1.16' \
  --learning_rate=1e-4 \
  --weight_l2=0.01 \
  --batch_size_per_device=24 \
  --iter_num=10 \
  --log_every_n_iter=10 \
  --data_dir="/dataset/bert/of_wiki_seq_len_128" \
  --data_part_num=1 \
  --model_load_dir="/dataset/model_zoo/bert_new_snapshot/of_L-12_H-768_A-12_random_init" \
  --model_save_dir="snapshots" \
  --seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_hidden_layers=12 \
  --num_attention_heads=12 \
  --max_position_embeddings=512 \
  --type_vocab_size=2 \
  --vocab_size=30522 \
  --attention_probs_dropout_prob=0.1 \
  --hidden_dropout_prob=0.1 \
  --hidden_size_per_head=64
