syntax = "proto2";
package oneflow;

import "oneflow/core/common/data_type.proto";
import "oneflow/core/operator/op_conf.proto";
import "oneflow/core/job/dlnet_conf.proto";
import "oneflow/core/job/placement.proto";
import "oneflow/core/job/sbp_parallel.proto";

message TrainConf {
  required int64 batch_size = 1; // batch_size % piece_size = 0
  required NormalModelUpdateOpUserConf model_update_conf = 3;
  required int32 num_of_batches_in_snapshot = 5;
  repeated string loss_lbn = 6;

  optional InitializerConf default_initializer_conf = 100;
  required float primary_lr = 101;
  optional float secondary_lr = 102 [default = -1];
  optional float weight_l1 = 103 [default = 0];
  optional float bias_l1 = 104 [default = 0];
  optional float weight_l2 = 105 [default = 0];
  optional float bias_l2 = 106 [default = 0];
  optional int64 piece_num_of_print_loss = 107 [default = -1];
  optional int64 piece_num_of_print_accuracy = 108 [default = -1];
}

message PredictConf {
  optional TrainConf tmp_split_fw_bw_train_conf = 1;
}

message SbpConf {
  map<string, SbpSignature> op_name2sbp_signature_conf = 1;
}

message ExperimentalRunConf {
  optional int64 piece_num_of_experiment_phase = 1 [default = -1];
  optional bool enable_experiment_run = 2 [default = false];
}

message Config {
  oneof JobType {
    TrainConf train_conf = 1;
    PredictConf predict_conf = 2;
  }
  required int64 piece_size = 3;
  required int32 data_part_num = 4; // piece_size % data_part_num = 0
  required int64 total_batch_num = 5;
  optional DataType default_data_type = 6 [default = kFloat]; // kFloat or kDouble
  optional int32 max_data_id_length = 7 [default = 0];

  optional ExperimentalRunConf exp_run_conf = 100;

  optional bool enable_cudnn = 200 [default = true];
  optional int64 cudnn_buf_limit_mbyte = 201 [default = 1024];  // 1GByte

  optional bool enable_mem_sharing = 300 [default = true];
  optional bool enable_inplace = 301 [default = true];
  optional bool enable_blob_mem_sharing = 302 [default = true];

  optional bool enable_nccl = 400 [default = true];
  optional bool use_nccl_inter_node_communication = 401 [default = false];

  optional int64 all_reduce_group_num = 500 [default = 8];
  // example:
  //   all_reduce_lazy_ratio = 0.5
  // It means that half of all_reduce nodes overlap with the forward pass of next batch
  optional float all_reduce_lazy_ratio = 600 [default = 0.5];
  optional int64 all_reduce_group_min_mbyte = 601 [default = 16];
  // example:
  //   total weight bytes is 1024M
  //   all_reduce_group_num = 8
  //   all_reduce_group_min_mbyte = 16
  //   all_reduce_group_size_warmup = 2
  // Each nodes' weight size are [16, 32, 64, 128, 128, 128, 128, 128, 128, 128, 16].
  // You can see the actual number of reduce group is slightly bigger than all_reduce_group_num.
  optional float all_reduce_group_size_warmup = 602 [default = 2];
  optional bool all_reduce_fp16 = 603 [default = true];

  optional int64 concurrency_width = 1000 [default = 128];
}

message JobConf {
  required DLNetConf net = 1;
  required Placement placement = 2;
  optional SbpConf sbp_conf = 3;
  required Config other = 4;
  required string job_name = 5;
  repeated string arg_op_name = 6;
}
