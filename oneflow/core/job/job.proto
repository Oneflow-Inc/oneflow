syntax = "proto2";
package oneflow;

import "oneflow/core/common/data_type.proto";
import "oneflow/core/job/dlnet_conf.proto";
import "oneflow/core/job/placement.proto";
import "oneflow/core/register/logical_blob_id.proto";
import "oneflow/core/register/op_blob_arg.proto";
import "oneflow/core/register/blob_desc.proto";
import "oneflow/core/operator/op_conf.proto";
import "oneflow/core/common/shape.proto";
import "oneflow/core/job/sbp_parallel.proto";
import "oneflow/core/framework/user_op_attr.proto";

message TrainConf {
  required NormalModelUpdateOpUserConf model_update_conf = 3;
  repeated string loss_lbn = 6;
  optional int32 loss_scale_factor = 7 [default = 1];
  optional string train_step_lbn = 8;
  optional string primary_lr_lbn = 9;
  optional string secondary_lr_lbn = 10;

  required float primary_lr = 101;
  optional float secondary_lr = 102;
}

message PredictConf {
}

message SbpConf {
  map<string, SbpSignature> op_name2sbp_signature_conf = 1;
}

message ExperimentalRunConf {
  optional int64 piece_num_of_experiment_phase = 1 [default = -1];
  optional bool enable_experiment_run = 2 [default = false];
}

message MemoryAllocationAlgorithmConf {
  optional bool use_mem_size_first_algo = 1 [default = true];
  optional bool use_mutual_exclusion_first_algo = 2 [default = true];
  optional bool use_time_line_algo = 3 [default = false];
}

message XrtConfig {
  message XlaConfig {
    // TODO
  }
  message TensorRTConfig {
    optional bool use_fp16 = 1 [default = false];
    optional bool use_int8 = 2 [default = false];
  }
  optional bool use_xla_jit = 1 [default = false];
  optional bool use_tensorrt = 2 [default = false];
  optional XlaConfig xla_config = 3;
  optional TensorRTConfig tensorrt_config = 4;
}

message IndexedSlicesOptimizerConf {
  optional bool enable = 1 [default = true];
  required OpNameSet include_op_names = 2;
}

message JobConfigProto {
  required string job_name = 1;

  oneof job_type {
    TrainConf train_conf = 3;
    PredictConf predict_conf = 4;
  }
  optional int64 total_batch_num = 7 [default = 1];
  optional DataType default_data_type = 8 [default = kFloat]; // kFloat or kDouble
  optional int32 max_data_id_length = 9 [default = 0];
  oneof default_initialize_conf {
    InitializerConf default_initializer_conf = 10;
    string default_initialize_with_snapshot_path = 11;
  }

  optional ExperimentalRunConf exp_run_conf = 100;
  optional bool use_memory_allocation_algorithm_v2 = 101 [default = true];
  optional MemoryAllocationAlgorithmConf memory_allocation_algorithm_conf = 102;

  optional XrtConfig xrt_config = 103;

  optional IndexedSlicesOptimizerConf indexed_slices_optimizer_conf = 104;

  optional bool enable_cudnn = 200 [default = true];
  optional int64 cudnn_buf_limit_mbyte = 201 [default = 1024];  // 1GByte
  optional int32 cudnn_conv_force_fwd_algo = 202;
  optional int32 cudnn_conv_force_bwd_data_algo = 203;
  optional int32 cudnn_conv_force_bwd_filter_algo = 204;
  optional bool cudnn_conv_heuristic_search_algo = 205 [default = true];
  optional bool cudnn_conv_use_deterministic_algo_only = 206 [default = false];
  
  optional bool enable_reuse_mem = 300 [default = true];
  optional bool enable_inplace = 301 [default = true];
  optional bool enable_inplace_in_reduce_struct = 302 [default = true];

  optional bool enable_nccl = 400 [default = true];
  optional bool use_nccl_inter_node_communication = 401 [default = true];
  optional bool use_boxing_v2 = 402 [default = false];

  optional bool enable_all_reduce_group = 500 [default = true];
  optional int64 all_reduce_group_num = 501 [default = 8];
  // example:
  //   all_reduce_lazy_ratio = 0.5
  // It means that half of all_reduce nodes overlap with the forward pass of next batch
  optional float all_reduce_lazy_ratio = 502 [default = 0.5];
  optional int64 all_reduce_group_min_mbyte = 503 [default = 16];
  // example:
  //   total weight bytes is 1024M
  //   all_reduce_group_num = 8
  //   all_reduce_group_min_mbyte = 16
  //   all_reduce_group_size_warmup = 2
  // Each nodes' weight size are [16, 32, 64, 128, 128, 128, 128, 128, 128, 128, 16].
  // You can see the actual number of reduce group is slightly bigger than all_reduce_group_num.
  optional float all_reduce_group_size_warmup = 504 [default = 2];
  optional bool all_reduce_fp16 = 505 [default = true];
  optional bool enable_non_distributed_optimizer = 506 [default = false];
  optional int64 non_distributed_optimizer_group_size_mbyte = 507 [default = 100];
  optional bool disable_all_reduce_sequence = 508 [default = false];
  optional bool prune_parallel_cast_ops = 509 [default = false];

  optional bool cudnn_conv_enable_pseudo_half = 600 [default = false];
  optional bool enable_float_compute_for_half_gemm = 601 [default = true];
  optional bool enable_auto_mixed_precision = 602 [default = false];
  
  optional bool enable_keep_header_only = 700 [default = false];

  optional int64 concurrency_width = 1000 [default = 128];

  map<string, UserOpAttrVal> flag_name2flag_value = 2000;
}

message OpTimeShape {
  optional ShapeProto in_blob_fastest_time_shape = 1;
  optional ShapeProto out_blob_time_shape = 2;
}

message ParallelBlobConf {
  required BlobDescProto logical_blob_desc_conf = 1;
  required ParallelConf parallel_conf = 2;
  required SbpParallel sbp_conf = 3;
  required OptInt64 batch_axis = 4;
}

message JobHelperConf {
  map<string, LogicalBlobIdPairs> tag2lbi_relations = 1;
  map<string, OpNameRelations> tag2op_name_relations = 2;
  map<string, OpTimeShape> op_name2op_time_shape = 3;
  map<string, BlobDescProto> lbn2logical_blob_desc = 4;
  map<string, OptInt64> lbn2batch_axis = 6;
  optional OpBlobArgPairs identical_sbp_oba_pairs = 7;
}

message Job {
  required DLNetConf net = 1;
  required Placement placement = 2;
  required JobConfigProto job_conf = 3;
  optional SbpConf sbp_conf = 4;
  optional JobHelperConf helper = 5;
}
